{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS269-NLG-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWhlrN5oJC7pSK3zAqYjwU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11ace850c27241f2934effedc83bf4ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e1ccc7aaf05c4867acb7c4f88e99dc0a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_50bfa24ba64949ab9b9378eba822a828",
              "IPY_MODEL_e9b4a6de99774b30a75500be7d141293"
            ]
          }
        },
        "e1ccc7aaf05c4867acb7c4f88e99dc0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50bfa24ba64949ab9b9378eba822a828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6e58be8eff6744ab894a35ab9308a8c0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28940,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28940,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5204db29e6f474b90d0bb2d1c7e8d19"
          }
        },
        "e9b4a6de99774b30a75500be7d141293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eaed599acb2546019135a4828f716ffa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.9k/28.9k [00:01&lt;00:00, 27.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae0a83155f694767b89945c2a15d6bf1"
          }
        },
        "6e58be8eff6744ab894a35ab9308a8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5204db29e6f474b90d0bb2d1c7e8d19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eaed599acb2546019135a4828f716ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae0a83155f694767b89945c2a15d6bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdb121ac354c496889ee221ec4f6056d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec8401e7ff0140fd8fc1554fc5ee97a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a39ab086f2e64fc1a24a9b97313319c6",
              "IPY_MODEL_e20b9cf268fc4fbdb290be3505b467dd"
            ]
          }
        },
        "ec8401e7ff0140fd8fc1554fc5ee97a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a39ab086f2e64fc1a24a9b97313319c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_57295c84b3874a59b13edf973c64a44c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 30329,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30329,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfb85d45c2614e0db5cdf71cd4334796"
          }
        },
        "e20b9cf268fc4fbdb290be3505b467dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_300e6d26fdeb4f7686116af23471ab40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30.3k/30.3k [00:00&lt;00:00, 568kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5bddcf50530944bebf141e21eb2a17bc"
          }
        },
        "57295c84b3874a59b13edf973c64a44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfb85d45c2614e0db5cdf71cd4334796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "300e6d26fdeb4f7686116af23471ab40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5bddcf50530944bebf141e21eb2a17bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f4aa11b208e49d3a3527608b585db52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ac4186b99e8f4d12a3458d4dcf758a83",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_353c294172134197a0a9194d7abec5c4",
              "IPY_MODEL_02edac2ba7c54786a5c2e38127fba08f"
            ]
          }
        },
        "ac4186b99e8f4d12a3458d4dcf758a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "353c294172134197a0a9194d7abec5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc8525091d584901aa75fa342f0262d5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7439277,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7439277,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4189299c11ed49678ac810df0c598a4a"
          }
        },
        "02edac2ba7c54786a5c2e38127fba08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9663ea91783e4e93872be3fba0368db0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7.44M/7.44M [00:00&lt;00:00, 18.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dbd9238504b470ca2fa0a604860768b"
          }
        },
        "bc8525091d584901aa75fa342f0262d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4189299c11ed49678ac810df0c598a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9663ea91783e4e93872be3fba0368db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dbd9238504b470ca2fa0a604860768b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c504db9290b4266a7bf0603d7119bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6cbba2d339744931b460f4429aa9d9c6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_60b72f07991d430e84d733ab0a16a22f",
              "IPY_MODEL_6a411ff098704c27b1634669e4d1dc6b"
            ]
          }
        },
        "6cbba2d339744931b460f4429aa9d9c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60b72f07991d430e84d733ab0a16a22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_29273a9ead0c4c7da6c126ab9608f003",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf827077ffc94ca99683985de7b29c96"
          }
        },
        "6a411ff098704c27b1634669e4d1dc6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6b8debc5269c418b814c5a403ff89cfc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 67349/0 [00:01&lt;00:00, 43614.74 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c095f82a29064c5fb4625840a757b7d1"
          }
        },
        "29273a9ead0c4c7da6c126ab9608f003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf827077ffc94ca99683985de7b29c96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b8debc5269c418b814c5a403ff89cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c095f82a29064c5fb4625840a757b7d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3958feadb56d47db9e7567dcb4d75034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fa29eda9b4d948aa9f77a29844b129f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f8bbef229ab8487c8dc0026379d2cad1",
              "IPY_MODEL_19f603b7b47741a28f3ed0c64fe11989"
            ]
          }
        },
        "fa29eda9b4d948aa9f77a29844b129f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8bbef229ab8487c8dc0026379d2cad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9e55419e2424f85a0c254a9fe1619a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f38cc9923e77485da03bb0897f7b2690"
          }
        },
        "19f603b7b47741a28f3ed0c64fe11989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e655e32c8164bd580fe0fdf7db5e5cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 872/0 [00:00&lt;00:00, 14733.99 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fdf74453a3a54a758041402110b8c566"
          }
        },
        "e9e55419e2424f85a0c254a9fe1619a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f38cc9923e77485da03bb0897f7b2690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e655e32c8164bd580fe0fdf7db5e5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fdf74453a3a54a758041402110b8c566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77ff96d0cbdc421da6f3c4b7c152b64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_36d14ad8c684488d840349fe8b84ca2c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cb765f50da17443f8ead766340ad431a",
              "IPY_MODEL_e0a451bc521c49a288824b473841205c"
            ]
          }
        },
        "36d14ad8c684488d840349fe8b84ca2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb765f50da17443f8ead766340ad431a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bf17519fc8294e2eb39cd01ff5b52044",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb625228040f4f14bd51f3ff50ad4c9c"
          }
        },
        "e0a451bc521c49a288824b473841205c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e63a5b6d2f8d479092d57275ae8cffe0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1821/0 [00:00&lt;00:00, 18953.79 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b024c21fc1ed4f72886c42138939f27d"
          }
        },
        "bf17519fc8294e2eb39cd01ff5b52044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb625228040f4f14bd51f3ff50ad4c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e63a5b6d2f8d479092d57275ae8cffe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b024c21fc1ed4f72886c42138939f27d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chong-z/NLG-project/blob/master/CS269_NLG_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8npvE8zXZTCl"
      },
      "source": [
        "# CS269 NLG Project: Generating Semi-Restricted Natural Language Adversarial Examples\n",
        "Group Member: Chong Zhang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29gLB7-fZemO"
      },
      "source": [
        "## Setup\n",
        "Install dependencies and clone the repo. May take a few minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1rXH4kSYLkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8c9455-c4cc-4d3f-8b20-653bbe55706a"
      },
      "source": [
        "!pip install pytorch-pretrained-bert==0.6.2 nlp torch nltk numpy tensorboardX pandas lm-scorer\n",
        "\n",
        "# lm-scorer installs a different version of transformers.\n",
        "!pip install transformers==3.0.2\n",
        "\n",
        "!git clone https://github.com/chong-z/NLG-project.git\n",
        "%cd NLG-project\n",
        "!sh dowloaddata.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 13.4MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert==0.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 48.9MB/s \n",
            "\u001b[?25hCollecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 50.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/9c/544396572c05841b7a2482c88be5dd54dcd18ba97abeb1e8d34daf921a54/boto3-1.16.30-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.3MB/s \n",
            "\u001b[?25hCollecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 204kB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Collecting botocore<1.20.0,>=1.19.30\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/a3/1ee497faf994d180df5d14d456eef1ef46ca1ffce617816faa4ff8164608/botocore-1.19.30-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 45.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=775fb36299985a57de23d72f6a8d24d60721c9a5695cb8c2fc58d24aa3fad921\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.30 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, pyarrow, xxhash, nlp, tensorboardX\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed boto3-1.16.30 botocore-1.19.30 jmespath-0.10.0 nlp-0.4.0 pyarrow-2.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tensorboardX-2.1 tokenizers-0.8.1rc1 transformers-3.0.2 xxhash-2.0.0\n",
            "Cloning into 'NLG-project'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (179/179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 179 (delta 92), reused 141 (delta 67), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (179/179), 31.55 MiB | 36.09 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "/content/NLG-project\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2020-12-07 21:18:00--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  4.52MB/s    in 8.0s    \n",
            "\n",
            "2020-12-07 21:18:09 (4.15 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVy8sLKdd3is"
      },
      "source": [
        "## Explore SST-2\n",
        "We focus the study on the SST-2 dataset. Here is a peek on the training examples.\n",
        "\n",
        "Label 0 and 1 denotes negative and positive sentiment, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930,
          "referenced_widgets": [
            "11ace850c27241f2934effedc83bf4ae",
            "e1ccc7aaf05c4867acb7c4f88e99dc0a",
            "50bfa24ba64949ab9b9378eba822a828",
            "e9b4a6de99774b30a75500be7d141293",
            "6e58be8eff6744ab894a35ab9308a8c0",
            "d5204db29e6f474b90d0bb2d1c7e8d19",
            "eaed599acb2546019135a4828f716ffa",
            "ae0a83155f694767b89945c2a15d6bf1",
            "cdb121ac354c496889ee221ec4f6056d",
            "ec8401e7ff0140fd8fc1554fc5ee97a9",
            "a39ab086f2e64fc1a24a9b97313319c6",
            "e20b9cf268fc4fbdb290be3505b467dd",
            "57295c84b3874a59b13edf973c64a44c",
            "cfb85d45c2614e0db5cdf71cd4334796",
            "300e6d26fdeb4f7686116af23471ab40",
            "5bddcf50530944bebf141e21eb2a17bc",
            "9f4aa11b208e49d3a3527608b585db52",
            "ac4186b99e8f4d12a3458d4dcf758a83",
            "353c294172134197a0a9194d7abec5c4",
            "02edac2ba7c54786a5c2e38127fba08f",
            "bc8525091d584901aa75fa342f0262d5",
            "4189299c11ed49678ac810df0c598a4a",
            "9663ea91783e4e93872be3fba0368db0",
            "2dbd9238504b470ca2fa0a604860768b",
            "2c504db9290b4266a7bf0603d7119bbe",
            "6cbba2d339744931b460f4429aa9d9c6",
            "60b72f07991d430e84d733ab0a16a22f",
            "6a411ff098704c27b1634669e4d1dc6b",
            "29273a9ead0c4c7da6c126ab9608f003",
            "cf827077ffc94ca99683985de7b29c96",
            "6b8debc5269c418b814c5a403ff89cfc",
            "c095f82a29064c5fb4625840a757b7d1",
            "3958feadb56d47db9e7567dcb4d75034",
            "fa29eda9b4d948aa9f77a29844b129f8",
            "f8bbef229ab8487c8dc0026379d2cad1",
            "19f603b7b47741a28f3ed0c64fe11989",
            "e9e55419e2424f85a0c254a9fe1619a7",
            "f38cc9923e77485da03bb0897f7b2690",
            "2e655e32c8164bd580fe0fdf7db5e5cb",
            "fdf74453a3a54a758041402110b8c566",
            "77ff96d0cbdc421da6f3c4b7c152b64b",
            "36d14ad8c684488d840349fe8b84ca2c",
            "cb765f50da17443f8ead766340ad431a",
            "e0a451bc521c49a288824b473841205c",
            "bf17519fc8294e2eb39cd01ff5b52044",
            "eb625228040f4f14bd51f3ff50ad4c9c",
            "e63a5b6d2f8d479092d57275ae8cffe0",
            "b024c21fc1ed4f72886c42138939f27d"
          ]
        },
        "id": "tq31xGKvd8HI",
        "outputId": "44e4aaba-7627-461a-cad2-ade1edac024f"
      },
      "source": [
        "import nlp\n",
        "import pandas as pd\n",
        "pd.options.display.min_rows = 20\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "sst2_data = nlp.load_dataset('glue', 'sst2')['train']\n",
        "df = pd.DataFrame(sst2_data)\n",
        "display(df)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11ace850c27241f2934effedc83bf4ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28940.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdb121ac354c496889ee221ec4f6056d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=30329.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown sizetotal: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f4aa11b208e49d3a3527608b585db52",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7439277.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c504db9290b4266a7bf0603d7119bbe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3958feadb56d47db9e7567dcb4d75034",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77ff96d0cbdc421da6f3c4b7c152b64b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>contains no wit , only labored gags</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>that loves its characters and communicates something rather beautiful about human nature</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>remains utterly satisfied to remain the same throughout</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the filmmakers could dredge up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>that 's far too tragic to merit such superficial treatment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>of saucy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>a depressed fifteen-year-old 's suicidal poetry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>are more deeply thought through than in most ` right-thinking ' films</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67339</th>\n",
              "      <td>67339</td>\n",
              "      <td>1</td>\n",
              "      <td>works more often than it does n't .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67340</th>\n",
              "      <td>67340</td>\n",
              "      <td>1</td>\n",
              "      <td>at least passably</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67341</th>\n",
              "      <td>67341</td>\n",
              "      <td>0</td>\n",
              "      <td>i also believe that resident evil is not it .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67342</th>\n",
              "      <td>67342</td>\n",
              "      <td>0</td>\n",
              "      <td>seem to be in a contest to see who can out-bad-act the other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67343</th>\n",
              "      <td>67343</td>\n",
              "      <td>1</td>\n",
              "      <td>showing off his doctorate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67344</th>\n",
              "      <td>67344</td>\n",
              "      <td>1</td>\n",
              "      <td>a delightful comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67345</th>\n",
              "      <td>67345</td>\n",
              "      <td>0</td>\n",
              "      <td>anguish , anger and frustration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67346</th>\n",
              "      <td>67346</td>\n",
              "      <td>1</td>\n",
              "      <td>at achieving the modest , crowd-pleasing goals it sets for itself</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67347</th>\n",
              "      <td>67347</td>\n",
              "      <td>1</td>\n",
              "      <td>a patient viewer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67348</th>\n",
              "      <td>67348</td>\n",
              "      <td>0</td>\n",
              "      <td>this new jangle of noise , mayhem and stupidity must be a serious contender for the title .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67349 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         idx  ...                                                                                                                                               sentence\n",
              "0          0  ...                                                                                                           hide new secretions from the parental units \n",
              "1          1  ...                                                                                                                   contains no wit , only labored gags \n",
              "2          2  ...                                                              that loves its characters and communicates something rather beautiful about human nature \n",
              "3          3  ...                                                                                               remains utterly satisfied to remain the same throughout \n",
              "4          4  ...                                                                              on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
              "5          5  ...                                                                                            that 's far too tragic to merit such superficial treatment \n",
              "6          6  ...  demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \n",
              "7          7  ...                                                                                                                                              of saucy \n",
              "8          8  ...                                                                                                       a depressed fifteen-year-old 's suicidal poetry \n",
              "9          9  ...                                                                                 are more deeply thought through than in most ` right-thinking ' films \n",
              "...      ...  ...                                                                                                                                                    ...\n",
              "67339  67339  ...                                                                                                                   works more often than it does n't . \n",
              "67340  67340  ...                                                                                                                                     at least passably \n",
              "67341  67341  ...                                                                                                         i also believe that resident evil is not it . \n",
              "67342  67342  ...                                                                                          seem to be in a contest to see who can out-bad-act the other \n",
              "67343  67343  ...                                                                                                                             showing off his doctorate \n",
              "67344  67344  ...                                                                                                                                   a delightful comedy \n",
              "67345  67345  ...                                                                                                                       anguish , anger and frustration \n",
              "67346  67346  ...                                                                                     at achieving the modest , crowd-pleasing goals it sets for itself \n",
              "67347  67347  ...                                                                                                                                      a patient viewer \n",
              "67348  67348  ...                                                           this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \n",
              "\n",
              "[67349 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p_ntouCbV-X"
      },
      "source": [
        "## Run adversarial attacks\n",
        "`semi_attack.py` is the main attack script. It takes in a few parameters:\n",
        "\n",
        " - `-c models/sample-GRU/E9.pytorch`: Use our pre-trained GRU VAE for generating interpolations. Please refer to the next section on how to train your own VAE.\n",
        " - `--iter 2`: Use 2 iterations for the 'binary' search.\n",
        " - `--steps 10`: Sample 10 interpolations per iteration.\n",
        " - `--victim_model \"distilbert-base-uncased-finetuned-sst-2-english\"`: Use the pre-trained [\"distilbert-base-uncased-finetuned-sst-2-english\"](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model from HuggingFace. Other models such as \"textattack/roberta-base-SST-2\" can also be used.\n",
        " - `--victim_sentence \"i study at ucla\"`: The victim sentence under attack. Our goal is to find an adversarial sentence similar but with different prediction than the victim sentence.\n",
        " - `--reference_sentence \"i finished my final exam at ucla\"`: The reference sentence providing the hint for the desired style.\n",
        " - Please refer to the script for additional parameters.\n",
        "\n",
        " \n",
        " ### Example 1\n",
        "\n",
        " In this example, the VAE only generates movie reviews despite the input (as expected). However, they do share some similarities with the victim and the reference sentence. For instance, they usually starts with an \"i\" and have the similar length.\n",
        "\n",
        " The final output is:\n",
        " ```\n",
        " -------Attack Result-------\n",
        "Victim Sentence: i study at ucla pred:0.8251549005508423\n",
        "Best Adv Sentence: this woefully hackneyed movie with flailing bodily movements <eos> pred:0.00024062106967903674\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LAOJJMrcUOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab42c5f-cabd-4e2c-c66b-5c49ae3ea190"
      },
      "source": [
        "!python semi_attack.py -c models/sample-GRU/E9.pytorch --iter 2 --steps 10 --rseed 7 --most_similar -v \\\n",
        "  --victim_model \"distilbert-base-uncased-finetuned-sst-2-english\" \\\n",
        "  --victim_sentence \"i study at ucla\" \\\n",
        "  --reference_sentence \"i finished my final exam at ucla\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-07 21:18:21.078838: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "VALID preprocessed file not found at data/ptb.valid.json. Creating new.\n",
            "Model loaded from models/sample-GRU/E9.pytorch\n",
            "Downloading: 100% 629/629 [00:00<00:00, 813kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 15.5MB/s]\n",
            "Downloading: 100% 268M/268M [00:03<00:00, 67.0MB/s]\n",
            "\n",
            "-------Initial Inputs-------\n",
            "Victim Sentence: i study at ucla pred:0.8251549005508423\n",
            "Reference Sentence: i finished my final exam at ucla pred:0.07655958086252213\n",
            "\n",
            "-------ITERATION 0-------\n",
            "Best Adv Sentence: i finished my final exam at ucla pred:0.07655958086252213\n",
            "-------PREDICTIONS-------\n",
            "0.825 & i study at ucla \\\\\n",
            "0.000 & a movie filled with unlikable , spiteful idiots  \\\\\n",
            "1.000 & a movie that will enthrall the whole family  \\\\\n",
            "0.003 & i ' ve seen before i saw this movie ,  \\\\\n",
            "0.000 & i saw this movie , i think it ' s just another crime movie  \\\\\n",
            "0.001 & i saw this movie with a taste for exaggeration  \\\\\n",
            "0.994 & i saw this movie with a taste  \\\\\n",
            "0.982 & i saw this movie with it  \\\\\n",
            "0.992 & i saw this movie  \\\\\n",
            "0.077 & i finished my final exam at ucla \\\\\n",
            "\n",
            "-------ITERATION 1-------\n",
            "Best Adv Sentence: a movie filled with unlikable , spiteful idiots <eos> pred:0.00024062106967903674\n",
            "-------PREDICTIONS-------\n",
            "0.825 & i study at ucla \\\\\n",
            "0.000 & this woefully hackneyed movie with flailing bodily movements  \\\\\n",
            "1.000 & a delicious crime drama  \\\\\n",
            "1.000 & a delicious , quirky movie  \\\\\n",
            "0.000 & a movie filled with unlikable , spiteful idiots  \\\\\n",
            "-------Attack Result-------\n",
            "Victim Sentence: i study at ucla pred:0.8251549005508423\n",
            "Best Adv Sentence: this woefully hackneyed movie with flailing bodily movements <eos> pred:0.00024062106967903674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHNdnQ6PHZ0b"
      },
      "source": [
        "### Example 2\n",
        "In this example, we pass in appropriate movie reviews to the script. We find adversarial examples in 2 iterations:\n",
        "\n",
        "1. In the first iteration, our method finds the sentence `an intriguing story , but ultimately purposeless , ...`, which has a different prediction than the victim example. We use it ass the reference example for the next iteration.\n",
        "2. After the second iteration, our method outputs the best adversarial example `the story is bogus and directed by joel ...`, which also has a different prediction but even closer to the victim sentence.\n",
        "\n",
        "The final output is:\n",
        "```\n",
        "-------Attack Result-------\n",
        "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688\n",
        "Best Adv Sentence: the story is bogus and directed by joel schumacher and a half dozen young men who has been overexposed , redolent of the plot device <eos> pred:0.0007369006052613258\n",
        "```\n",
        "\n",
        "Please note that we may need additional iterations to find a more similar adversarial example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBWAP3MjZKm0",
        "outputId": "6b78f725-833a-46e5-ebf2-8ac995c753c6"
      },
      "source": [
        "!python semi_attack.py -c models/sample-GRU/E9.pytorch --iter 2 --steps 10 --rseed 3 --most_similar -v \\\n",
        "  --victim_model \"distilbert-base-uncased-finetuned-sst-2-english\" \\\n",
        "  --victim_sentence \"a strangely compelling and brilliantly acted psychological drama .\" \\\n",
        "  --reference_sentence \"an absurdist sitcom about alienation , separation and loss .\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-07 21:18:48.481691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Model loaded from models/sample-GRU/E9.pytorch\n",
            "\n",
            "-------Initial Inputs-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688\n",
            "Reference Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "\n",
            "-------ITERATION 0-------\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . \\\\\n",
            "0.999 & a quietly introspective portrait of pure misogynist evil  \\\\\n",
            "1.000 & a quietly moving portrait of an intelligent screenplay  \\\\\n",
            "0.999 & an intriguing story , but ultimately purposeless and satisfying heroine  \\\\\n",
            "0.001 & an intriguing story , but ultimately purposeless , and ultimately empty examination of the modern rut of the entire scenario .  \\\\\n",
            "1.000 & an intriguing story , but the story simply because it is a refreshingly forthright one .  \\\\\n",
            "0.003 & an absurdist sitcom about alienation , separation and loss . \\\\\n",
            "\n",
            "-------ITERATION 1-------\n",
            "Best Adv Sentence: an intriguing story , but ultimately purposeless , and ultimately empty examination of the modern rut of the entire scenario . <eos> pred:0.0007369006052613258\n",
            "-------PREDICTIONS-------\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . \\\\\n",
            "1.000 & a delicious crime drama that revives the free-wheeling noir spirit of its spirit with its own cuteness  \\\\\n",
            "1.000 & a delicious crime drama that manages to invest its target audience talked the experience of its own ironic implications .  \\\\\n",
            "1.000 & a delicious crime drama that manages to invest its target audience talked the buoyant energy of surprise .  \\\\\n",
            "0.953 & a quietly introspective portrait of a subculture hell-bent on the emptiness of creating a screenplay  \\\\\n",
            "1.000 & the premise of a handsome and well-made entertainment  \\\\\n",
            "0.001 & the story is bogus and directed by joel schumacher and a half dozen young men who has been overexposed , redolent of the plot device  \\\\\n",
            "0.001 & the story is bogus and directed by joel schumacher and a half dozen young men who has been overexposed , redolent of the plot device .  \\\\\n",
            "1.000 & the story is pushed into the margins of a handsome and well-made entertainment .  \\\\\n",
            "0.237 & the awkwardly paced and the euphoria of the pool with its exquisite acting , inventive screenplay and listless direction .  \\\\\n",
            "0.001 & an intriguing story , but ultimately purposeless , and ultimately empty examination of the modern rut of the entire scenario .  \\\\\n",
            "-------Attack Result-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688\n",
            "Best Adv Sentence: the story is bogus and directed by joel schumacher and a half dozen young men who has been overexposed , redolent of the plot device <eos> pred:0.0007369006052613258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nASttNMrKBmD"
      },
      "source": [
        "## Train VAE\n",
        "Train a LSTM-based VAE with 10 epoches and default settings, may take half an hour to run. Please refer to `train.py` for additional parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQKOVHcTKD5W",
        "outputId": "3c528d79-e64e-466f-9169-cd9665f4ebe4"
      },
      "source": [
        "!python train.py --data_dir data --epochs 10 --rnn_type lstm -tb"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 478750579/478750579 [00:10<00:00, 47853190.10B/s]\n",
            "100% 656/656 [00:00<00:00, 626614.31B/s]\n",
            "100% 815973/815973 [00:00<00:00, 28416838.83B/s]\n",
            "100% 458495/458495 [00:00<00:00, 18688156.93B/s]\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
            "TRAIN preprocessed file not found at data/ptb.train.json. Creating new.\n",
            "2020-12-07 21:43:48.096056: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Vocablurary of 14221 keys created.\n",
            "SentenceVAE(\n",
            "  (embedding): Embedding(14221, 300)\n",
            "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
            "  (encoder_rnn): LSTM(300, 256, batch_first=True)\n",
            "  (decoder_rnn): LSTM(300, 256, batch_first=True)\n",
            "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
            "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
            "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
            "  (outputs2vocab): Linear(in_features=256, out_features=14221, bias=True)\n",
            ")\n",
            "TRAIN Batch 0000/2104, Loss  117.1735, NLL-Loss  117.1733, KL-Loss    0.0940, KL-Weight  0.002, Perp-loss    0.0000, Perp-weight 519.013\n",
            "TRAIN Batch 0050/2104, Loss   56.2679, NLL-Loss   56.1980, KL-Loss   32.0163, KL-Weight  0.002, Perp-loss    0.0000, Perp-weight 458.145\n",
            "TRAIN Batch 0100/2104, Loss   68.9362, NLL-Loss   68.8908, KL-Loss   18.3773, KL-Weight  0.002, Perp-loss    0.0000, Perp-weight 404.429\n",
            "TRAIN Batch 0150/2104, Loss   55.6245, NLL-Loss   55.5623, KL-Loss   22.2133, KL-Weight  0.003, Perp-loss    0.0000, Perp-weight 357.025\n",
            "TRAIN Batch 0200/2104, Loss   69.1101, NLL-Loss   69.0164, KL-Loss   29.5419, KL-Weight  0.003, Perp-loss    0.0000, Perp-weight 315.191\n",
            "TRAIN Batch 0250/2104, Loss   57.9804, NLL-Loss   57.8665, KL-Loss   31.6928, KL-Weight  0.004, Perp-loss    0.0000, Perp-weight 278.272\n",
            "TRAIN Batch 0300/2104, Loss   75.7385, NLL-Loss   75.5599, KL-Loss   43.8708, KL-Weight  0.004, Perp-loss    0.0000, Perp-weight 245.692\n",
            "TRAIN Batch 0350/2104, Loss   57.6136, NLL-Loss   57.4446, KL-Loss   36.6496, KL-Weight  0.005, Perp-loss    0.0000, Perp-weight 216.940\n",
            "TRAIN Batch 0400/2104, Loss   71.8752, NLL-Loss   71.6358, KL-Loss   45.8511, KL-Weight  0.005, Perp-loss    0.0000, Perp-weight 191.566\n",
            "TRAIN Batch 0450/2104, Loss   46.8487, NLL-Loss   46.6387, KL-Loss   35.5357, KL-Weight  0.006, Perp-loss    0.0000, Perp-weight 169.174\n",
            "TRAIN Batch 0500/2104, Loss   68.1306, NLL-Loss   67.8702, KL-Loss   38.9052, KL-Weight  0.007, Perp-loss    0.0000, Perp-weight 149.413\n",
            "TRAIN Batch 0550/2104, Loss   58.0463, NLL-Loss   57.7946, KL-Loss   33.2193, KL-Weight  0.008, Perp-loss    0.0000, Perp-weight 131.974\n",
            "TRAIN Batch 0600/2104, Loss   65.3999, NLL-Loss   65.0730, KL-Loss   38.1189, KL-Weight  0.009, Perp-loss    0.0000, Perp-weight 116.584\n",
            "TRAIN Batch 0650/2104, Loss   51.6520, NLL-Loss   51.3406, KL-Loss   32.0679, KL-Weight  0.010, Perp-loss    0.0000, Perp-weight 103.003\n",
            "TRAIN Batch 0700/2104, Loss   49.7676, NLL-Loss   49.4129, KL-Loss   32.2837, KL-Weight  0.011, Perp-loss    0.0000, Perp-weight 91.017\n",
            "TRAIN Batch 0750/2104, Loss   63.6862, NLL-Loss   63.2773, KL-Loss   32.8918, KL-Weight  0.012, Perp-loss    0.0000, Perp-weight 80.440\n",
            "TRAIN Batch 0800/2104, Loss   45.4556, NLL-Loss   45.0320, KL-Loss   30.1187, KL-Weight  0.014, Perp-loss    0.0000, Perp-weight 71.105\n",
            "TRAIN Batch 0850/2104, Loss   56.8253, NLL-Loss   56.3737, KL-Loss   28.3909, KL-Weight  0.016, Perp-loss    0.0000, Perp-weight 62.868\n",
            "TRAIN Batch 0900/2104, Loss   47.0390, NLL-Loss   46.5338, KL-Loss   28.0899, KL-Weight  0.018, Perp-loss    0.0000, Perp-weight 55.598\n",
            "TRAIN Batch 0950/2104, Loss   58.3619, NLL-Loss   57.7779, KL-Loss   28.7269, KL-Weight  0.020, Perp-loss    0.0000, Perp-weight 49.183\n",
            "TRAIN Batch 1000/2104, Loss   60.9053, NLL-Loss   60.2478, KL-Loss   28.6147, KL-Weight  0.023, Perp-loss    0.0000, Perp-weight 43.521\n",
            "TRAIN Batch 1050/2104, Loss   62.4329, NLL-Loss   61.6506, KL-Loss   30.1382, KL-Weight  0.026, Perp-loss    0.0000, Perp-weight 38.525\n",
            "TRAIN Batch 1100/2104, Loss   62.9773, NLL-Loss   62.1483, KL-Loss   28.2801, KL-Weight  0.029, Perp-loss    0.0000, Perp-weight 34.115\n",
            "TRAIN Batch 1150/2104, Loss   49.5661, NLL-Loss   48.7420, KL-Loss   24.9100, KL-Weight  0.033, Perp-loss    0.0000, Perp-weight 30.224\n",
            "TRAIN Batch 1200/2104, Loss   53.4327, NLL-Loss   52.4484, KL-Loss   26.3684, KL-Weight  0.037, Perp-loss    0.0000, Perp-weight 26.790\n",
            "TRAIN Batch 1250/2104, Loss   41.5582, NLL-Loss   40.6225, KL-Loss   22.2333, KL-Weight  0.042, Perp-loss    0.0000, Perp-weight 23.760\n",
            "TRAIN Batch 1300/2104, Loss   52.4263, NLL-Loss   51.3708, KL-Loss   22.2556, KL-Weight  0.047, Perp-loss    0.0000, Perp-weight 21.086\n",
            "TRAIN Batch 1350/2104, Loss   60.0992, NLL-Loss   58.7947, KL-Loss   24.4270, KL-Weight  0.053, Perp-loss    0.0000, Perp-weight 18.725\n",
            "TRAIN Batch 1400/2104, Loss   48.2092, NLL-Loss   46.9877, KL-Loss   20.3284, KL-Weight  0.060, Perp-loss    0.0000, Perp-weight 16.643\n",
            "TRAIN Batch 1450/2104, Loss   53.4667, NLL-Loss   52.1071, KL-Loss   20.1284, KL-Weight  0.068, Perp-loss    0.0000, Perp-weight 14.805\n",
            "TRAIN Batch 1500/2104, Loss   46.1805, NLL-Loss   44.8617, KL-Loss   17.3850, KL-Weight  0.076, Perp-loss    0.0000, Perp-weight 13.182\n",
            "TRAIN Batch 1550/2104, Loss   58.9334, NLL-Loss   57.3340, KL-Loss   18.7945, KL-Weight  0.085, Perp-loss    0.0000, Perp-weight 11.751\n",
            "TRAIN Batch 1600/2104, Loss   58.8384, NLL-Loss   57.0783, KL-Loss   18.4597, KL-Weight  0.095, Perp-loss    0.0000, Perp-weight 10.488\n",
            "TRAIN Batch 1650/2104, Loss   56.7078, NLL-Loss   54.7654, KL-Loss   18.2058, KL-Weight  0.107, Perp-loss    0.0000, Perp-weight  9.373\n",
            "TRAIN Batch 1700/2104, Loss   48.5429, NLL-Loss   46.7235, KL-Loss   15.2636, KL-Weight  0.119, Perp-loss    0.0000, Perp-weight  8.389\n",
            "TRAIN Batch 1750/2104, Loss   46.4791, NLL-Loss   44.4535, KL-Loss   15.2339, KL-Weight  0.133, Perp-loss    0.0000, Perp-weight  7.521\n",
            "TRAIN Batch 1800/2104, Loss   56.6764, NLL-Loss   54.4957, KL-Loss   14.7296, KL-Weight  0.148, Perp-loss    0.0000, Perp-weight  6.755\n",
            "TRAIN Batch 1850/2104, Loss   64.6702, NLL-Loss   62.1939, KL-Loss   15.0516, KL-Weight  0.165, Perp-loss    0.0000, Perp-weight  6.078\n",
            "TRAIN Batch 1900/2104, Loss   49.2279, NLL-Loss   47.0085, KL-Loss   12.1658, KL-Weight  0.182, Perp-loss    0.0000, Perp-weight  5.482\n",
            "TRAIN Batch 1950/2104, Loss   40.1334, NLL-Loss   38.0165, KL-Loss   10.4896, KL-Weight  0.202, Perp-loss    0.0000, Perp-weight  4.955\n",
            "TRAIN Batch 2000/2104, Loss   51.2603, NLL-Loss   48.3996, KL-Loss   12.8455, KL-Weight  0.223, Perp-loss    0.0000, Perp-weight  4.490\n",
            "TRAIN Batch 2050/2104, Loss   48.7541, NLL-Loss   46.1086, KL-Loss   10.7944, KL-Weight  0.245, Perp-loss    0.0000, Perp-weight  4.080\n",
            "TRAIN Batch 2100/2104, Loss   45.3760, NLL-Loss   42.5630, KL-Loss   10.4595, KL-Weight  0.269, Perp-loss    0.0000, Perp-weight  3.718\n",
            "TRAIN Batch 2104/2104, Loss   70.9126, NLL-Loss   67.6704, KL-Loss   11.9678, KL-Weight  0.271, Perp-loss    0.0000, Perp-weight  3.691\n",
            "TRAIN Epoch 00/10, Mean ELBO   57.6613\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E0.pytorch\n",
            "VALID Batch 0000/27, Loss  107.2146, NLL-Loss  103.0076, KL-Loss   15.5010, KL-Weight  0.271, Perp-loss    0.0000, Perp-weight  3.685\n",
            "VALID Batch 0027/27, Loss   86.8678, NLL-Loss   82.8126, KL-Loss   14.9414, KL-Weight  0.271, Perp-loss    0.0000, Perp-weight  3.685\n",
            "VALID Epoch 00/10, Mean ELBO  111.7014\n",
            "TRAIN Batch 0000/2104, Loss   49.6764, NLL-Loss   46.8638, KL-Loss   10.3628, KL-Weight  0.271, Perp-loss    0.0000, Perp-weight  3.685\n",
            "TRAIN Batch 0050/2104, Loss   61.0010, NLL-Loss   57.4840, KL-Loss   11.8493, KL-Weight  0.297, Perp-loss    0.0000, Perp-weight  3.369\n",
            "TRAIN Batch 0100/2104, Loss   49.0961, NLL-Loss   45.7749, KL-Loss   10.2647, KL-Weight  0.324, Perp-loss    0.0000, Perp-weight  3.091\n",
            "TRAIN Batch 0150/2104, Loss   49.8472, NLL-Loss   46.7330, KL-Loss    8.8601, KL-Weight  0.351, Perp-loss    0.0000, Perp-weight  2.845\n",
            "TRAIN Batch 0200/2104, Loss   69.8218, NLL-Loss   65.8480, KL-Loss   10.4443, KL-Weight  0.380, Perp-loss    0.0000, Perp-weight  2.628\n",
            "TRAIN Batch 0250/2104, Loss   53.6047, NLL-Loss   50.2948, KL-Loss    8.0660, KL-Weight  0.410, Perp-loss    0.0000, Perp-weight  2.437\n",
            "TRAIN Batch 0300/2104, Loss   39.2068, NLL-Loss   36.0041, KL-Loss    7.2641, KL-Weight  0.441, Perp-loss    0.0000, Perp-weight  2.268\n",
            "TRAIN Batch 0350/2104, Loss   46.6639, NLL-Loss   43.3560, KL-Loss    7.0097, KL-Weight  0.472, Perp-loss    0.0000, Perp-weight  2.119\n",
            "TRAIN Batch 0400/2104, Loss   43.0798, NLL-Loss   39.7640, KL-Loss    6.5905, KL-Weight  0.503, Perp-loss    0.0000, Perp-weight  1.988\n",
            "TRAIN Batch 0450/2104, Loss   54.9723, NLL-Loss   51.0667, KL-Loss    7.3095, KL-Weight  0.534, Perp-loss    0.0000, Perp-weight  1.872\n",
            "TRAIN Batch 0500/2104, Loss   53.0121, NLL-Loss   49.1440, KL-Loss    6.8432, KL-Weight  0.565, Perp-loss    0.0000, Perp-weight  1.769\n",
            "TRAIN Batch 0550/2104, Loss   52.0545, NLL-Loss   48.6347, KL-Loss    5.7410, KL-Weight  0.596, Perp-loss    0.0000, Perp-weight  1.679\n",
            "TRAIN Batch 0600/2104, Loss   43.2328, NLL-Loss   40.0507, KL-Loss    5.0882, KL-Weight  0.625, Perp-loss    0.0000, Perp-weight  1.599\n",
            "TRAIN Batch 0650/2104, Loss   53.1544, NLL-Loss   49.7925, KL-Loss    5.1391, KL-Weight  0.654, Perp-loss    0.0000, Perp-weight  1.529\n",
            "TRAIN Batch 0700/2104, Loss   62.7326, NLL-Loss   59.0061, KL-Loss    5.4649, KL-Weight  0.682, Perp-loss    0.0000, Perp-weight  1.466\n",
            "TRAIN Batch 0750/2104, Loss   60.8005, NLL-Loss   57.0257, KL-Loss    5.3288, KL-Weight  0.708, Perp-loss    0.0000, Perp-weight  1.412\n",
            "TRAIN Batch 0800/2104, Loss   66.6938, NLL-Loss   62.4966, KL-Loss    5.7221, KL-Weight  0.734, Perp-loss    0.0000, Perp-weight  1.363\n",
            "TRAIN Batch 0850/2104, Loss   52.5416, NLL-Loss   49.1486, KL-Loss    4.4809, KL-Weight  0.757, Perp-loss    0.0000, Perp-weight  1.321\n",
            "TRAIN Batch 0900/2104, Loss   45.5794, NLL-Loss   41.9929, KL-Loss    4.6013, KL-Weight  0.779, Perp-loss    0.0000, Perp-weight  1.283\n",
            "TRAIN Batch 0950/2104, Loss   38.0596, NLL-Loss   34.7148, KL-Loss    4.1800, KL-Weight  0.800, Perp-loss    0.0000, Perp-weight  1.250\n",
            "TRAIN Batch 1000/2104, Loss   44.6134, NLL-Loss   41.2222, KL-Loss    4.1385, KL-Weight  0.819, Perp-loss    0.0000, Perp-weight  1.220\n",
            "TRAIN Batch 1050/2104, Loss   53.6806, NLL-Loss   50.3454, KL-Loss    3.9838, KL-Weight  0.837, Perp-loss    0.0000, Perp-weight  1.194\n",
            "TRAIN Batch 1100/2104, Loss   48.7274, NLL-Loss   45.3318, KL-Loss    3.9784, KL-Weight  0.854, Perp-loss    0.0000, Perp-weight  1.172\n",
            "TRAIN Batch 1150/2104, Loss   41.0311, NLL-Loss   38.2327, KL-Loss    3.2222, KL-Weight  0.868, Perp-loss    0.0000, Perp-weight  1.151\n",
            "TRAIN Batch 1200/2104, Loss   48.6939, NLL-Loss   45.4282, KL-Loss    3.7022, KL-Weight  0.882, Perp-loss    0.0000, Perp-weight  1.134\n",
            "TRAIN Batch 1250/2104, Loss   56.3934, NLL-Loss   53.1286, KL-Loss    3.6499, KL-Weight  0.894, Perp-loss    0.0000, Perp-weight  1.118\n",
            "TRAIN Batch 1300/2104, Loss   55.4413, NLL-Loss   52.1728, KL-Loss    3.6087, KL-Weight  0.906, Perp-loss    0.0000, Perp-weight  1.104\n",
            "TRAIN Batch 1350/2104, Loss   52.3609, NLL-Loss   49.1093, KL-Loss    3.5503, KL-Weight  0.916, Perp-loss    0.0000, Perp-weight  1.092\n",
            "TRAIN Batch 1400/2104, Loss   41.9588, NLL-Loss   39.3583, KL-Loss    2.8113, KL-Weight  0.925, Perp-loss    0.0000, Perp-weight  1.081\n",
            "TRAIN Batch 1450/2104, Loss   53.7822, NLL-Loss   50.4770, KL-Loss    3.5416, KL-Weight  0.933, Perp-loss    0.0000, Perp-weight  1.072\n",
            "TRAIN Batch 1500/2104, Loss   54.7146, NLL-Loss   51.6012, KL-Loss    3.3099, KL-Weight  0.941, Perp-loss    0.0000, Perp-weight  1.063\n",
            "TRAIN Batch 1550/2104, Loss   40.5940, NLL-Loss   38.0185, KL-Loss    2.7190, KL-Weight  0.947, Perp-loss    0.0000, Perp-weight  1.056\n",
            "TRAIN Batch 1600/2104, Loss   41.0539, NLL-Loss   38.4342, KL-Loss    2.7486, KL-Weight  0.953, Perp-loss    0.0000, Perp-weight  1.049\n",
            "TRAIN Batch 1650/2104, Loss   53.2799, NLL-Loss   50.3500, KL-Loss    3.0571, KL-Weight  0.958, Perp-loss    0.0000, Perp-weight  1.043\n",
            "TRAIN Batch 1700/2104, Loss   47.1389, NLL-Loss   44.4041, KL-Loss    2.8395, KL-Weight  0.963, Perp-loss    0.0000, Perp-weight  1.038\n",
            "TRAIN Batch 1750/2104, Loss   47.2344, NLL-Loss   44.5766, KL-Loss    2.7477, KL-Weight  0.967, Perp-loss    0.0000, Perp-weight  1.034\n",
            "TRAIN Batch 1800/2104, Loss   47.7791, NLL-Loss   44.8492, KL-Loss    3.0173, KL-Weight  0.971, Perp-loss    0.0000, Perp-weight  1.030\n",
            "TRAIN Batch 1850/2104, Loss   57.9097, NLL-Loss   55.2653, KL-Loss    2.7140, KL-Weight  0.974, Perp-loss    0.0000, Perp-weight  1.026\n",
            "TRAIN Batch 1900/2104, Loss   46.9599, NLL-Loss   44.5419, KL-Loss    2.4742, KL-Weight  0.977, Perp-loss    0.0000, Perp-weight  1.023\n",
            "TRAIN Batch 1950/2104, Loss   37.8476, NLL-Loss   35.4237, KL-Loss    2.4736, KL-Weight  0.980, Perp-loss    0.0000, Perp-weight  1.020\n",
            "TRAIN Batch 2000/2104, Loss   43.6712, NLL-Loss   40.9532, KL-Loss    2.7672, KL-Weight  0.982, Perp-loss    0.0000, Perp-weight  1.018\n",
            "TRAIN Batch 2050/2104, Loss   49.6279, NLL-Loss   46.8060, KL-Loss    2.8669, KL-Weight  0.984, Perp-loss    0.0000, Perp-weight  1.016\n",
            "TRAIN Batch 2100/2104, Loss   48.1203, NLL-Loss   45.2890, KL-Loss    2.8712, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "TRAIN Batch 2104/2104, Loss   50.8856, NLL-Loss   48.2484, KL-Loss    2.6740, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "TRAIN Epoch 01/10, Mean ELBO   49.7286\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E1.pytorch\n",
            "VALID Batch 0000/27, Loss  113.5661, NLL-Loss  109.8393, KL-Loss    3.7787, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "VALID Batch 0027/27, Loss   92.4172, NLL-Loss   88.5644, KL-Loss    3.9064, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "VALID Epoch 01/10, Mean ELBO  117.5791\n",
            "TRAIN Batch 0000/2104, Loss   47.3852, NLL-Loss   44.8140, KL-Loss    2.6070, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "TRAIN Batch 0050/2104, Loss   47.2862, NLL-Loss   44.6180, KL-Loss    2.7009, KL-Weight  0.988, Perp-loss    0.0000, Perp-weight  1.012\n",
            "TRAIN Batch 0100/2104, Loss   41.4383, NLL-Loss   38.7221, KL-Loss    2.7456, KL-Weight  0.989, Perp-loss    0.0000, Perp-weight  1.011\n",
            "TRAIN Batch 0150/2104, Loss   51.7569, NLL-Loss   48.9425, KL-Loss    2.8414, KL-Weight  0.991, Perp-loss    0.0000, Perp-weight  1.010\n",
            "TRAIN Batch 0200/2104, Loss   38.5209, NLL-Loss   36.1854, KL-Loss    2.3553, KL-Weight  0.992, Perp-loss    0.0000, Perp-weight  1.008\n",
            "TRAIN Batch 0250/2104, Loss   41.5036, NLL-Loss   38.9920, KL-Loss    2.5303, KL-Weight  0.993, Perp-loss    0.0000, Perp-weight  1.007\n",
            "TRAIN Batch 0300/2104, Loss   49.5783, NLL-Loss   47.1397, KL-Loss    2.4546, KL-Weight  0.993, Perp-loss    0.0000, Perp-weight  1.007\n",
            "TRAIN Batch 0350/2104, Loss   53.0875, NLL-Loss   50.1574, KL-Loss    2.9471, KL-Weight  0.994, Perp-loss    0.0000, Perp-weight  1.006\n",
            "TRAIN Batch 0400/2104, Loss   44.1786, NLL-Loss   41.4835, KL-Loss    2.7089, KL-Weight  0.995, Perp-loss    0.0000, Perp-weight  1.005\n",
            "TRAIN Batch 0450/2104, Loss   56.9108, NLL-Loss   54.0537, KL-Loss    2.8700, KL-Weight  0.996, Perp-loss    0.0000, Perp-weight  1.005\n",
            "TRAIN Batch 0500/2104, Loss   42.8371, NLL-Loss   40.3394, KL-Loss    2.5076, KL-Weight  0.996, Perp-loss    0.0000, Perp-weight  1.004\n",
            "TRAIN Batch 0550/2104, Loss   38.9629, NLL-Loss   36.5576, KL-Loss    2.4137, KL-Weight  0.996, Perp-loss    0.0000, Perp-weight  1.004\n",
            "TRAIN Batch 0600/2104, Loss   37.5405, NLL-Loss   35.1747, KL-Loss    2.3731, KL-Weight  0.997, Perp-loss    0.0000, Perp-weight  1.003\n",
            "TRAIN Batch 0650/2104, Loss   36.6040, NLL-Loss   34.1064, KL-Loss    2.5044, KL-Weight  0.997, Perp-loss    0.0000, Perp-weight  1.003\n",
            "TRAIN Batch 0700/2104, Loss   46.2805, NLL-Loss   43.8550, KL-Loss    2.4314, KL-Weight  0.998, Perp-loss    0.0000, Perp-weight  1.002\n",
            "TRAIN Batch 0750/2104, Loss   40.8366, NLL-Loss   38.4818, KL-Loss    2.3598, KL-Weight  0.998, Perp-loss    0.0000, Perp-weight  1.002\n",
            "TRAIN Batch 0800/2104, Loss   65.3586, NLL-Loss   62.4652, KL-Loss    2.8989, KL-Weight  0.998, Perp-loss    0.0000, Perp-weight  1.002\n",
            "TRAIN Batch 0850/2104, Loss   48.2397, NLL-Loss   45.5075, KL-Loss    2.7367, KL-Weight  0.998, Perp-loss    0.0000, Perp-weight  1.002\n",
            "TRAIN Batch 0900/2104, Loss   35.6489, NLL-Loss   33.2446, KL-Loss    2.4078, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 0950/2104, Loss   50.0604, NLL-Loss   47.4460, KL-Loss    2.6177, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1000/2104, Loss   54.5166, NLL-Loss   51.9976, KL-Loss    2.5219, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1050/2104, Loss   49.3507, NLL-Loss   46.8099, KL-Loss    2.5433, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1100/2104, Loss   33.1886, NLL-Loss   30.9006, KL-Loss    2.2901, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1150/2104, Loss   57.7022, NLL-Loss   54.8227, KL-Loss    2.8818, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1200/2104, Loss   40.3548, NLL-Loss   38.1725, KL-Loss    2.1838, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1250/2104, Loss   48.6568, NLL-Loss   46.1442, KL-Loss    2.5141, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1300/2104, Loss   36.1341, NLL-Loss   34.0147, KL-Loss    2.1205, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1350/2104, Loss   40.9719, NLL-Loss   38.6532, KL-Loss    2.3198, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   41.2027, NLL-Loss   38.9467, KL-Loss    2.2570, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   41.7704, NLL-Loss   39.4065, KL-Loss    2.3647, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   41.8768, NLL-Loss   39.8079, KL-Loss    2.0696, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   52.3698, NLL-Loss   49.7449, KL-Loss    2.6257, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   46.7968, NLL-Loss   44.2846, KL-Loss    2.5128, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   55.1291, NLL-Loss   52.5202, KL-Loss    2.6094, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   46.2249, NLL-Loss   43.9599, KL-Loss    2.2654, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   44.7273, NLL-Loss   42.3816, KL-Loss    2.3461, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   42.0196, NLL-Loss   39.6728, KL-Loss    2.3471, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   44.1236, NLL-Loss   41.6369, KL-Loss    2.4871, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   40.8746, NLL-Loss   38.6870, KL-Loss    2.1879, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   45.8372, NLL-Loss   43.4631, KL-Loss    2.3743, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   39.6130, NLL-Loss   37.4430, KL-Loss    2.1702, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   45.1557, NLL-Loss   42.6710, KL-Loss    2.4848, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   38.4473, NLL-Loss   36.3303, KL-Loss    2.1171, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   36.3322, NLL-Loss   34.1882, KL-Loss    2.1442, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 02/10, Mean ELBO   44.3241\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E2.pytorch\n",
            "VALID Batch 0000/27, Loss  115.3552, NLL-Loss  112.0046, KL-Loss    3.3508, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss   94.4314, NLL-Loss   90.8373, KL-Loss    3.5944, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 02/10, Mean ELBO  120.0328\n",
            "TRAIN Batch 0000/2104, Loss   42.9144, NLL-Loss   40.5588, KL-Loss    2.3558, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   44.2999, NLL-Loss   42.0061, KL-Loss    2.2939, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   33.8153, NLL-Loss   31.7758, KL-Loss    2.0396, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   39.8725, NLL-Loss   37.4116, KL-Loss    2.4610, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   50.7141, NLL-Loss   48.0834, KL-Loss    2.6308, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   42.9680, NLL-Loss   40.6749, KL-Loss    2.2932, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   49.7747, NLL-Loss   47.2601, KL-Loss    2.5147, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   39.8069, NLL-Loss   37.3319, KL-Loss    2.4751, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   45.4181, NLL-Loss   42.9702, KL-Loss    2.4480, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   43.8030, NLL-Loss   41.4889, KL-Loss    2.3141, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   37.8704, NLL-Loss   35.6508, KL-Loss    2.2196, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   29.4507, NLL-Loss   27.4881, KL-Loss    1.9626, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   37.8084, NLL-Loss   35.5301, KL-Loss    2.2783, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   43.2505, NLL-Loss   40.8853, KL-Loss    2.3652, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   47.6883, NLL-Loss   45.3827, KL-Loss    2.3056, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   48.0498, NLL-Loss   45.5941, KL-Loss    2.4557, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   38.3695, NLL-Loss   36.0716, KL-Loss    2.2979, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   43.9107, NLL-Loss   41.3771, KL-Loss    2.5337, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   39.8662, NLL-Loss   37.6610, KL-Loss    2.2052, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   36.0663, NLL-Loss   33.9896, KL-Loss    2.0767, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   36.3484, NLL-Loss   34.1217, KL-Loss    2.2267, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   48.1757, NLL-Loss   45.7613, KL-Loss    2.4144, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   46.5788, NLL-Loss   44.2896, KL-Loss    2.2892, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   34.0640, NLL-Loss   32.0959, KL-Loss    1.9681, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   51.2736, NLL-Loss   49.0044, KL-Loss    2.2692, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   41.6746, NLL-Loss   39.4344, KL-Loss    2.2402, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   36.4789, NLL-Loss   34.3287, KL-Loss    2.1503, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   47.3405, NLL-Loss   44.8699, KL-Loss    2.4706, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   47.6173, NLL-Loss   45.2901, KL-Loss    2.3272, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   42.2273, NLL-Loss   40.1229, KL-Loss    2.1044, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   33.8527, NLL-Loss   31.8435, KL-Loss    2.0092, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   44.4807, NLL-Loss   42.2179, KL-Loss    2.2628, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   39.9568, NLL-Loss   37.7812, KL-Loss    2.1755, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   36.2480, NLL-Loss   34.0289, KL-Loss    2.2190, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   46.6308, NLL-Loss   44.4311, KL-Loss    2.1997, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   42.3325, NLL-Loss   40.1797, KL-Loss    2.1527, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   39.2896, NLL-Loss   37.2478, KL-Loss    2.0418, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   40.9083, NLL-Loss   38.7634, KL-Loss    2.1449, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   56.1933, NLL-Loss   53.6328, KL-Loss    2.5605, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   38.0522, NLL-Loss   35.9900, KL-Loss    2.0622, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   44.0012, NLL-Loss   41.7682, KL-Loss    2.2330, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   35.5861, NLL-Loss   33.6962, KL-Loss    1.8899, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   44.5289, NLL-Loss   42.2140, KL-Loss    2.3149, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   35.7567, NLL-Loss   33.7330, KL-Loss    2.0236, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 03/10, Mean ELBO   40.3278\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E3.pytorch\n",
            "VALID Batch 0000/27, Loss  117.1638, NLL-Loss  114.0176, KL-Loss    3.1462, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss   96.2346, NLL-Loss   93.1124, KL-Loss    3.1221, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 03/10, Mean ELBO  122.3865\n",
            "TRAIN Batch 0000/2104, Loss   35.0990, NLL-Loss   33.0724, KL-Loss    2.0265, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   38.3624, NLL-Loss   36.0915, KL-Loss    2.2709, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   38.6111, NLL-Loss   36.3680, KL-Loss    2.2432, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   35.4444, NLL-Loss   33.4765, KL-Loss    1.9679, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   42.6558, NLL-Loss   40.2407, KL-Loss    2.4151, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   35.7156, NLL-Loss   33.4677, KL-Loss    2.2478, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   37.9362, NLL-Loss   35.9257, KL-Loss    2.0105, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   40.3715, NLL-Loss   38.1899, KL-Loss    2.1816, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   37.7924, NLL-Loss   35.5493, KL-Loss    2.2431, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   35.9923, NLL-Loss   33.8285, KL-Loss    2.1638, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   36.2015, NLL-Loss   34.2852, KL-Loss    1.9163, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   35.7808, NLL-Loss   33.9505, KL-Loss    1.8303, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   34.8735, NLL-Loss   32.7530, KL-Loss    2.1205, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   47.1162, NLL-Loss   44.7706, KL-Loss    2.3456, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   44.7341, NLL-Loss   42.4521, KL-Loss    2.2820, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   41.9931, NLL-Loss   39.9659, KL-Loss    2.0272, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   28.2320, NLL-Loss   26.4205, KL-Loss    1.8116, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   34.7582, NLL-Loss   32.7031, KL-Loss    2.0551, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   35.2619, NLL-Loss   33.2567, KL-Loss    2.0051, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   35.6294, NLL-Loss   33.7475, KL-Loss    1.8819, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   37.4009, NLL-Loss   35.1171, KL-Loss    2.2838, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   31.7893, NLL-Loss   30.0619, KL-Loss    1.7274, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   38.1265, NLL-Loss   36.0479, KL-Loss    2.0786, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   35.5323, NLL-Loss   33.2033, KL-Loss    2.3291, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   43.4291, NLL-Loss   41.1447, KL-Loss    2.2845, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   48.4634, NLL-Loss   46.3093, KL-Loss    2.1541, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   43.0969, NLL-Loss   41.0695, KL-Loss    2.0275, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   36.3409, NLL-Loss   34.3946, KL-Loss    1.9463, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   44.2101, NLL-Loss   42.0446, KL-Loss    2.1655, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   36.0151, NLL-Loss   33.9835, KL-Loss    2.0316, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   37.4782, NLL-Loss   35.4987, KL-Loss    1.9795, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   29.2739, NLL-Loss   27.2998, KL-Loss    1.9741, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   40.7396, NLL-Loss   38.5136, KL-Loss    2.2260, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   29.7917, NLL-Loss   28.0291, KL-Loss    1.7626, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   42.9260, NLL-Loss   40.6799, KL-Loss    2.2461, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   40.5063, NLL-Loss   38.4288, KL-Loss    2.0775, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   36.8986, NLL-Loss   34.8649, KL-Loss    2.0336, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   29.7940, NLL-Loss   27.9527, KL-Loss    1.8412, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   37.6912, NLL-Loss   35.5707, KL-Loss    2.1205, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   36.7267, NLL-Loss   34.6123, KL-Loss    2.1144, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   32.4412, NLL-Loss   30.3385, KL-Loss    2.1027, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   46.2484, NLL-Loss   44.1078, KL-Loss    2.1407, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   40.4141, NLL-Loss   38.4082, KL-Loss    2.0059, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   38.3721, NLL-Loss   36.4547, KL-Loss    1.9173, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 04/10, Mean ELBO   37.4956\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E4.pytorch\n",
            "VALID Batch 0000/27, Loss  120.3867, NLL-Loss  117.1811, KL-Loss    3.2056, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss   96.7780, NLL-Loss   93.5223, KL-Loss    3.2557, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 04/10, Mean ELBO  125.2215\n",
            "TRAIN Batch 0000/2104, Loss   27.6024, NLL-Loss   25.6647, KL-Loss    1.9377, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   41.5527, NLL-Loss   39.4646, KL-Loss    2.0881, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   32.3552, NLL-Loss   30.3463, KL-Loss    2.0088, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   32.0606, NLL-Loss   29.9977, KL-Loss    2.0629, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   40.8640, NLL-Loss   38.7387, KL-Loss    2.1253, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   42.3034, NLL-Loss   40.0425, KL-Loss    2.2608, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   28.5861, NLL-Loss   26.5346, KL-Loss    2.0514, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   30.0074, NLL-Loss   27.9793, KL-Loss    2.0281, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   32.7693, NLL-Loss   30.8856, KL-Loss    1.8837, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   31.0578, NLL-Loss   29.0458, KL-Loss    2.0120, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   34.2008, NLL-Loss   32.0527, KL-Loss    2.1482, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   33.2505, NLL-Loss   31.2464, KL-Loss    2.0041, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   33.8120, NLL-Loss   31.6265, KL-Loss    2.1855, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   36.1012, NLL-Loss   34.1248, KL-Loss    1.9764, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   45.2019, NLL-Loss   43.1600, KL-Loss    2.0419, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   44.4718, NLL-Loss   42.3152, KL-Loss    2.1566, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   37.9773, NLL-Loss   35.7311, KL-Loss    2.2463, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   38.5137, NLL-Loss   36.5596, KL-Loss    1.9541, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   38.6436, NLL-Loss   36.5304, KL-Loss    2.1131, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   31.2379, NLL-Loss   29.4616, KL-Loss    1.7762, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   39.6388, NLL-Loss   37.5956, KL-Loss    2.0432, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   36.6567, NLL-Loss   34.6473, KL-Loss    2.0094, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   32.4641, NLL-Loss   30.4378, KL-Loss    2.0263, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   34.0388, NLL-Loss   32.1499, KL-Loss    1.8889, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   34.5573, NLL-Loss   32.7232, KL-Loss    1.8340, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   32.1222, NLL-Loss   30.2756, KL-Loss    1.8466, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   35.4206, NLL-Loss   33.4796, KL-Loss    1.9410, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   37.7100, NLL-Loss   35.7492, KL-Loss    1.9608, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   33.4095, NLL-Loss   31.4725, KL-Loss    1.9369, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   33.2875, NLL-Loss   31.3844, KL-Loss    1.9031, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   37.1196, NLL-Loss   35.0142, KL-Loss    2.1054, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   43.7295, NLL-Loss   41.5777, KL-Loss    2.1519, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   35.4499, NLL-Loss   33.5105, KL-Loss    1.9394, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   26.7341, NLL-Loss   24.9555, KL-Loss    1.7786, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   36.7392, NLL-Loss   34.7141, KL-Loss    2.0251, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   45.7356, NLL-Loss   43.5244, KL-Loss    2.2112, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   30.1561, NLL-Loss   28.3354, KL-Loss    1.8207, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   40.4856, NLL-Loss   38.2924, KL-Loss    2.1933, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   37.6259, NLL-Loss   35.6084, KL-Loss    2.0175, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   30.7960, NLL-Loss   29.0701, KL-Loss    1.7259, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   39.0943, NLL-Loss   37.0775, KL-Loss    2.0167, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   29.4197, NLL-Loss   27.5176, KL-Loss    1.9021, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   35.4151, NLL-Loss   33.2529, KL-Loss    2.1622, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   36.0474, NLL-Loss   33.9495, KL-Loss    2.0979, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 05/10, Mean ELBO   35.4085\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E5.pytorch\n",
            "VALID Batch 0000/27, Loss  121.9796, NLL-Loss  118.9925, KL-Loss    2.9871, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss   99.4635, NLL-Loss   96.4647, KL-Loss    2.9987, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 05/10, Mean ELBO  127.9411\n",
            "TRAIN Batch 0000/2104, Loss   30.4266, NLL-Loss   28.5230, KL-Loss    1.9036, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   30.5279, NLL-Loss   28.7698, KL-Loss    1.7580, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   28.6353, NLL-Loss   26.7288, KL-Loss    1.9065, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   35.0522, NLL-Loss   32.9854, KL-Loss    2.0667, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   31.4473, NLL-Loss   29.4295, KL-Loss    2.0179, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   34.8185, NLL-Loss   32.7553, KL-Loss    2.0632, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   33.6234, NLL-Loss   31.6968, KL-Loss    1.9265, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   35.2111, NLL-Loss   33.2626, KL-Loss    1.9486, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   38.0694, NLL-Loss   36.1478, KL-Loss    1.9216, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   31.4384, NLL-Loss   29.3841, KL-Loss    2.0542, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   31.0341, NLL-Loss   28.9612, KL-Loss    2.0729, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   39.7832, NLL-Loss   37.2709, KL-Loss    2.5124, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   36.3448, NLL-Loss   34.3154, KL-Loss    2.0294, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   27.6647, NLL-Loss   25.8216, KL-Loss    1.8431, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   32.2566, NLL-Loss   30.3691, KL-Loss    1.8874, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   26.9272, NLL-Loss   24.9775, KL-Loss    1.9497, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   28.6779, NLL-Loss   26.8872, KL-Loss    1.7907, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   37.8652, NLL-Loss   35.6920, KL-Loss    2.1732, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   28.4303, NLL-Loss   26.5730, KL-Loss    1.8573, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   31.4212, NLL-Loss   29.4051, KL-Loss    2.0160, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   40.6693, NLL-Loss   38.6726, KL-Loss    1.9967, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   32.2335, NLL-Loss   30.1551, KL-Loss    2.0784, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   33.5284, NLL-Loss   31.7135, KL-Loss    1.8149, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   42.7830, NLL-Loss   40.6736, KL-Loss    2.1093, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   24.8109, NLL-Loss   23.2718, KL-Loss    1.5392, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   30.8531, NLL-Loss   28.7715, KL-Loss    2.0815, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   36.9039, NLL-Loss   34.9241, KL-Loss    1.9798, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   37.6854, NLL-Loss   35.5170, KL-Loss    2.1685, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   29.3571, NLL-Loss   27.4976, KL-Loss    1.8595, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   34.2067, NLL-Loss   31.8876, KL-Loss    2.3191, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   31.1695, NLL-Loss   29.3481, KL-Loss    1.8215, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   31.5730, NLL-Loss   29.8823, KL-Loss    1.6907, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   33.8649, NLL-Loss   32.1244, KL-Loss    1.7405, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   36.8867, NLL-Loss   34.8012, KL-Loss    2.0855, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   32.1312, NLL-Loss   30.3271, KL-Loss    1.8041, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   41.4264, NLL-Loss   39.1228, KL-Loss    2.3036, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   33.6440, NLL-Loss   31.7459, KL-Loss    1.8982, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   33.6785, NLL-Loss   31.7729, KL-Loss    1.9056, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   36.5814, NLL-Loss   34.7404, KL-Loss    1.8409, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   34.7238, NLL-Loss   32.8801, KL-Loss    1.8437, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   36.4283, NLL-Loss   34.3149, KL-Loss    2.1134, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   31.2530, NLL-Loss   29.4463, KL-Loss    1.8067, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   33.2073, NLL-Loss   31.1506, KL-Loss    2.0567, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   29.6274, NLL-Loss   27.9402, KL-Loss    1.6872, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 06/10, Mean ELBO   33.7560\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E6.pytorch\n",
            "VALID Batch 0000/27, Loss  125.5246, NLL-Loss  122.5825, KL-Loss    2.9421, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss  101.2004, NLL-Loss   98.1121, KL-Loss    3.0883, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 06/10, Mean ELBO  130.5326\n",
            "TRAIN Batch 0000/2104, Loss   30.9520, NLL-Loss   28.7962, KL-Loss    2.1558, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   32.5132, NLL-Loss   30.8385, KL-Loss    1.6746, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   34.4086, NLL-Loss   32.5635, KL-Loss    1.8451, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   27.1595, NLL-Loss   25.3501, KL-Loss    1.8094, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   34.2038, NLL-Loss   32.2241, KL-Loss    1.9797, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   29.1444, NLL-Loss   27.3819, KL-Loss    1.7625, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   29.4484, NLL-Loss   27.6710, KL-Loss    1.7774, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   29.7914, NLL-Loss   27.8192, KL-Loss    1.9722, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   29.7099, NLL-Loss   27.7086, KL-Loss    2.0014, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   41.4717, NLL-Loss   39.3464, KL-Loss    2.1254, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   32.2322, NLL-Loss   30.2845, KL-Loss    1.9477, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   33.4223, NLL-Loss   31.4296, KL-Loss    1.9926, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   35.8813, NLL-Loss   33.9391, KL-Loss    1.9423, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   31.4518, NLL-Loss   29.4899, KL-Loss    1.9619, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   28.9020, NLL-Loss   27.1964, KL-Loss    1.7057, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   32.7607, NLL-Loss   30.7927, KL-Loss    1.9680, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   31.6972, NLL-Loss   29.7586, KL-Loss    1.9386, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   29.5469, NLL-Loss   27.5819, KL-Loss    1.9650, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   28.9825, NLL-Loss   27.1412, KL-Loss    1.8413, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   34.7351, NLL-Loss   32.9556, KL-Loss    1.7796, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   33.4447, NLL-Loss   31.5542, KL-Loss    1.8905, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   38.0798, NLL-Loss   36.1131, KL-Loss    1.9667, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   39.0204, NLL-Loss   37.0030, KL-Loss    2.0174, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   37.1214, NLL-Loss   35.2601, KL-Loss    1.8613, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   31.3516, NLL-Loss   29.3251, KL-Loss    2.0266, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   30.0639, NLL-Loss   28.1131, KL-Loss    1.9508, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   32.8028, NLL-Loss   30.9627, KL-Loss    1.8401, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   30.2796, NLL-Loss   28.5229, KL-Loss    1.7568, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   30.4658, NLL-Loss   28.5781, KL-Loss    1.8877, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   30.3944, NLL-Loss   28.4018, KL-Loss    1.9927, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   36.1942, NLL-Loss   34.2282, KL-Loss    1.9659, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   28.4633, NLL-Loss   26.7228, KL-Loss    1.7405, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   33.2044, NLL-Loss   31.2288, KL-Loss    1.9756, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   34.2041, NLL-Loss   32.2588, KL-Loss    1.9453, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   26.6229, NLL-Loss   24.9700, KL-Loss    1.6529, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   37.9695, NLL-Loss   35.7861, KL-Loss    2.1834, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   30.3645, NLL-Loss   28.5645, KL-Loss    1.8000, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   43.1808, NLL-Loss   41.0102, KL-Loss    2.1706, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   26.3343, NLL-Loss   24.7507, KL-Loss    1.5836, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   36.6599, NLL-Loss   34.7418, KL-Loss    1.9181, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   26.2888, NLL-Loss   24.4639, KL-Loss    1.8250, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   32.9109, NLL-Loss   30.9317, KL-Loss    1.9792, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   23.8445, NLL-Loss   22.0888, KL-Loss    1.7556, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   37.2382, NLL-Loss   35.4196, KL-Loss    1.8186, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 07/10, Mean ELBO   32.4204\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E7.pytorch\n",
            "VALID Batch 0000/27, Loss  127.8126, NLL-Loss  125.2820, KL-Loss    2.5306, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss  105.4563, NLL-Loss  102.8436, KL-Loss    2.6126, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 07/10, Mean ELBO  132.3727\n",
            "TRAIN Batch 0000/2104, Loss   28.1715, NLL-Loss   26.3892, KL-Loss    1.7823, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   32.3941, NLL-Loss   30.7445, KL-Loss    1.6496, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   30.7397, NLL-Loss   28.8031, KL-Loss    1.9365, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   31.1313, NLL-Loss   29.5003, KL-Loss    1.6310, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   28.1173, NLL-Loss   26.3315, KL-Loss    1.7857, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   36.0265, NLL-Loss   33.8720, KL-Loss    2.1545, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   32.2302, NLL-Loss   30.0065, KL-Loss    2.2237, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   34.2189, NLL-Loss   32.2506, KL-Loss    1.9683, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   30.3137, NLL-Loss   28.4844, KL-Loss    1.8294, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   25.1025, NLL-Loss   23.3792, KL-Loss    1.7233, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   27.5022, NLL-Loss   25.9204, KL-Loss    1.5818, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   33.5060, NLL-Loss   31.3992, KL-Loss    2.1068, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   33.0060, NLL-Loss   31.0402, KL-Loss    1.9658, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   28.7679, NLL-Loss   26.9812, KL-Loss    1.7867, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   28.5647, NLL-Loss   26.8121, KL-Loss    1.7526, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   36.1328, NLL-Loss   34.0261, KL-Loss    2.1067, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   34.3173, NLL-Loss   32.2509, KL-Loss    2.0664, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   30.5803, NLL-Loss   28.7855, KL-Loss    1.7948, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   33.9520, NLL-Loss   31.9828, KL-Loss    1.9692, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   34.1594, NLL-Loss   32.3457, KL-Loss    1.8138, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   34.1321, NLL-Loss   32.1858, KL-Loss    1.9463, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   26.5671, NLL-Loss   24.9805, KL-Loss    1.5866, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   27.0950, NLL-Loss   25.3028, KL-Loss    1.7921, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   33.4423, NLL-Loss   31.6369, KL-Loss    1.8055, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   26.1965, NLL-Loss   24.5007, KL-Loss    1.6958, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   30.2961, NLL-Loss   28.7261, KL-Loss    1.5700, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   28.0216, NLL-Loss   26.2364, KL-Loss    1.7852, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   33.6275, NLL-Loss   31.9680, KL-Loss    1.6595, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   28.5622, NLL-Loss   26.8090, KL-Loss    1.7532, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   26.6528, NLL-Loss   24.9623, KL-Loss    1.6906, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   32.0439, NLL-Loss   30.2292, KL-Loss    1.8147, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   31.4898, NLL-Loss   29.6376, KL-Loss    1.8523, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   34.1261, NLL-Loss   32.2739, KL-Loss    1.8522, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   30.1677, NLL-Loss   28.4759, KL-Loss    1.6917, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   27.4255, NLL-Loss   25.6519, KL-Loss    1.7735, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   31.5771, NLL-Loss   29.8196, KL-Loss    1.7575, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   27.8164, NLL-Loss   26.0991, KL-Loss    1.7172, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   35.3272, NLL-Loss   33.5289, KL-Loss    1.7983, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   29.1402, NLL-Loss   27.3152, KL-Loss    1.8250, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   26.4558, NLL-Loss   24.7961, KL-Loss    1.6597, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   27.4462, NLL-Loss   25.6177, KL-Loss    1.8285, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   30.3211, NLL-Loss   28.7336, KL-Loss    1.5875, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   29.6585, NLL-Loss   27.9846, KL-Loss    1.6739, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   35.2432, NLL-Loss   33.3859, KL-Loss    1.8573, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 08/10, Mean ELBO   31.2981\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E8.pytorch\n",
            "VALID Batch 0000/27, Loss  128.8703, NLL-Loss  126.1477, KL-Loss    2.7226, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss  103.3714, NLL-Loss  100.6485, KL-Loss    2.7230, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 08/10, Mean ELBO  134.7749\n",
            "TRAIN Batch 0000/2104, Loss   29.5090, NLL-Loss   27.6134, KL-Loss    1.8955, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   31.0194, NLL-Loss   29.1419, KL-Loss    1.8775, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   29.6847, NLL-Loss   27.9086, KL-Loss    1.7762, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   34.8969, NLL-Loss   33.0652, KL-Loss    1.8317, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   36.6363, NLL-Loss   34.3956, KL-Loss    2.2407, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   26.1739, NLL-Loss   24.4681, KL-Loss    1.7058, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   28.6240, NLL-Loss   26.8338, KL-Loss    1.7902, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   26.7790, NLL-Loss   25.1378, KL-Loss    1.6412, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   31.8706, NLL-Loss   30.1587, KL-Loss    1.7119, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   35.0529, NLL-Loss   32.9831, KL-Loss    2.0698, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   28.9356, NLL-Loss   27.0787, KL-Loss    1.8569, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   30.8643, NLL-Loss   29.0670, KL-Loss    1.7973, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   36.3041, NLL-Loss   34.4324, KL-Loss    1.8717, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   29.6860, NLL-Loss   27.7190, KL-Loss    1.9671, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   23.7429, NLL-Loss   22.1860, KL-Loss    1.5570, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   33.4390, NLL-Loss   31.4872, KL-Loss    1.9517, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   30.4824, NLL-Loss   28.7997, KL-Loss    1.6828, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   38.3541, NLL-Loss   36.2034, KL-Loss    2.1507, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   24.3906, NLL-Loss   22.6730, KL-Loss    1.7176, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   26.4070, NLL-Loss   24.6476, KL-Loss    1.7594, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   31.0506, NLL-Loss   29.2839, KL-Loss    1.7667, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   28.0263, NLL-Loss   26.2716, KL-Loss    1.7547, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   28.4364, NLL-Loss   26.7179, KL-Loss    1.7185, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   29.5249, NLL-Loss   27.9027, KL-Loss    1.6222, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   29.4543, NLL-Loss   27.5910, KL-Loss    1.8633, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   30.3257, NLL-Loss   28.5928, KL-Loss    1.7329, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   30.8751, NLL-Loss   29.1021, KL-Loss    1.7729, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   27.4032, NLL-Loss   25.6618, KL-Loss    1.7413, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   27.3406, NLL-Loss   25.6862, KL-Loss    1.6545, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   33.0086, NLL-Loss   31.2404, KL-Loss    1.7682, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   28.0013, NLL-Loss   26.0705, KL-Loss    1.9308, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   29.2229, NLL-Loss   27.3632, KL-Loss    1.8597, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   35.2966, NLL-Loss   33.4486, KL-Loss    1.8481, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   32.6661, NLL-Loss   30.8902, KL-Loss    1.7759, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   29.5235, NLL-Loss   27.5625, KL-Loss    1.9610, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   34.9164, NLL-Loss   33.0539, KL-Loss    1.8625, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   30.5429, NLL-Loss   28.6226, KL-Loss    1.9204, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   27.7888, NLL-Loss   26.1260, KL-Loss    1.6628, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   28.2220, NLL-Loss   26.4849, KL-Loss    1.7371, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   34.9342, NLL-Loss   33.0744, KL-Loss    1.8598, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   31.3062, NLL-Loss   29.4197, KL-Loss    1.8865, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   29.5456, NLL-Loss   27.6866, KL-Loss    1.8590, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   33.0266, NLL-Loss   31.3279, KL-Loss    1.6988, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   27.8360, NLL-Loss   26.0204, KL-Loss    1.8156, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 09/10, Mean ELBO   30.3749\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E9.pytorch\n",
            "VALID Batch 0000/27, Loss  131.4679, NLL-Loss  128.7295, KL-Loss    2.7384, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss  103.6126, NLL-Loss  100.8314, KL-Loss    2.7812, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 09/10, Mean ELBO  136.7958\n",
            "Total train time: 1151.4904823303223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-T51bhAMdSt"
      },
      "source": [
        "Test your model with an adversarial attack, please replace `bin/2020-Dec-07-21:43:33/E9.pytorch` with the path to your checkpoint as shown in the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhYAOMwmKNfI",
        "outputId": "9016b8da-22f8-40d3-dff0-62a37175091e"
      },
      "source": [
        "!python semi_attack.py -c bin/2020-Dec-07-21:43:33/E9.pytorch --rnn_type lstm --iter 2 --steps 10 --rseed 7 --most_similar -v \\\n",
        "  --victim_model \"distilbert-base-uncased-finetuned-sst-2-english\" \\\n",
        "  --victim_sentence \"a strangely compelling and brilliantly acted psychological drama .\" \\\n",
        "  --reference_sentence \"an absurdist sitcom about alienation , separation and loss .\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-07 22:35:09.916066: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Model loaded from bin/2020-Dec-07-21:43:33/E9.pytorch\n",
            "\n",
            "-------Initial Inputs-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688\n",
            "Reference Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "\n",
            "-------ITERATION 0-------\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . \\\\\n",
            "1.000 & , it ' s a very good yarn .  \\\\\n",
            "1.000 & , it ' s a very good viewing alternative .  \\\\\n",
            "0.998 & a fascinating curiosity piece of filmmaking  \\\\\n",
            "0.003 & an absurdist sitcom about alienation , separation and loss . \\\\\n",
            "\n",
            "-------ITERATION 1-------\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . \\\\\n",
            "0.999 & a fascinating curiosity piece of filmmaking , and the class warfare that embroils two surefire men  \\\\\n",
            "0.998 & a fascinating curiosity piece of filmmaking , and  \\\\\n",
            "0.998 & a fascinating curiosity piece of filmmaking ,  \\\\\n",
            "0.998 & a fascinating curiosity piece of filmmaking  \\\\\n",
            "0.003 & an absurdist sitcom about alienation , separation and loss . \\\\\n",
            "-------Attack Result-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}