{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS269-NLG-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNa0H5Ux56N4MT8SOByxB9W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d05062687ece451c8df7bcfa3cd5357d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c9564c049124fb18cee3930ee1a7f5b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8fe4b42bdce14da1ab34653e8d1280e5",
              "IPY_MODEL_894912b59609403f97f8d2f447a6fbea"
            ]
          }
        },
        "5c9564c049124fb18cee3930ee1a7f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fe4b42bdce14da1ab34653e8d1280e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8dda308c099542ff9ffc864fdcf0cc6b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28940,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28940,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf3af74a30894543845a28faab644db2"
          }
        },
        "894912b59609403f97f8d2f447a6fbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a55af8b5689417e9b6a8f949deddeea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.9k/28.9k [00:01&lt;00:00, 15.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a28f0e2d14094b88b3eb1335aa90c58e"
          }
        },
        "8dda308c099542ff9ffc864fdcf0cc6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf3af74a30894543845a28faab644db2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a55af8b5689417e9b6a8f949deddeea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a28f0e2d14094b88b3eb1335aa90c58e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96a8848b603e455c916d98908c7111fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3fd3335dc91f490387c2d7509cf39bf2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1d3b26a4d36b4391890770d79727f34a",
              "IPY_MODEL_1ec377b26cf545a2881819d026fc1404"
            ]
          }
        },
        "3fd3335dc91f490387c2d7509cf39bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d3b26a4d36b4391890770d79727f34a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a14ff1ba53be4d3cb52e523b37e8b33d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 30329,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30329,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a5152646f7b468ea08007226c932712"
          }
        },
        "1ec377b26cf545a2881819d026fc1404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d50c822d6b5741e3b52203a801037574",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30.3k/30.3k [00:00&lt;00:00, 351kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f73729b95b234405afedf02feca1a4e1"
          }
        },
        "a14ff1ba53be4d3cb52e523b37e8b33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a5152646f7b468ea08007226c932712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d50c822d6b5741e3b52203a801037574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f73729b95b234405afedf02feca1a4e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a35673f5bd4479f9a1e95cefa0f18a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_669d988f904c45058ed33142accc8446",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_003319d5b2914f269931b1712a480e19",
              "IPY_MODEL_018ee90f21df402daf4df1804e96d99a"
            ]
          }
        },
        "669d988f904c45058ed33142accc8446": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "003319d5b2914f269931b1712a480e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_727bfa3f55b94664b0c75ab287d8fed7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7439277,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7439277,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b5f587e56b94de88e6a6b8d70956e8a"
          }
        },
        "018ee90f21df402daf4df1804e96d99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f8878f5dc974edfa18ec6ca0d9f036d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7.44M/7.44M [00:00&lt;00:00, 19.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6db472cda6a240bf854934374646c454"
          }
        },
        "727bfa3f55b94664b0c75ab287d8fed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b5f587e56b94de88e6a6b8d70956e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f8878f5dc974edfa18ec6ca0d9f036d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6db472cda6a240bf854934374646c454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "995ebde83b644a938fd33f8a6ad67725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a18ef9c312f64645b5bf9843c442fb16",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e725e420d9904a33a3d1a768e85b56e2",
              "IPY_MODEL_4b03c24143084e2fb8a6543a00a7d27d"
            ]
          }
        },
        "a18ef9c312f64645b5bf9843c442fb16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e725e420d9904a33a3d1a768e85b56e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a2bceab5a6743248d0b4beed72f8324",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff26af976166491094a11fc079613f97"
          }
        },
        "4b03c24143084e2fb8a6543a00a7d27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83d307d8091949438802573dc4895a5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 67349/0 [00:01&lt;00:00, 48552.57 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_921a45e4863b47f3b196662439916ec3"
          }
        },
        "9a2bceab5a6743248d0b4beed72f8324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff26af976166491094a11fc079613f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83d307d8091949438802573dc4895a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "921a45e4863b47f3b196662439916ec3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6abca299ad4749d58fa054d824da4743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c27e468ae9e74f198a490463333d90e3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_036a1feccbce4427b5c99ffa000a2f34",
              "IPY_MODEL_f45cb0a7bc1649cca431e15c997b3fe1"
            ]
          }
        },
        "c27e468ae9e74f198a490463333d90e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "036a1feccbce4427b5c99ffa000a2f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_062451a616304eef8f3efdbbf0297f14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0956b9ed53514274a56f12d58933c635"
          }
        },
        "f45cb0a7bc1649cca431e15c997b3fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2bdecdde19e846569afa3062cc569c7d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 872/0 [00:00&lt;00:00, 14984.69 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1ee47d52f8c462c8c73e2e908408c4c"
          }
        },
        "062451a616304eef8f3efdbbf0297f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0956b9ed53514274a56f12d58933c635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bdecdde19e846569afa3062cc569c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1ee47d52f8c462c8c73e2e908408c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a184875193e645b3864ff224ec8a9425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1c272ba3a467473eb23ff78c11006ba5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6f209ea2ab83403e86a3e1c441629a9a",
              "IPY_MODEL_ae497f7df0f3470c9625b77aa6b7312b"
            ]
          }
        },
        "1c272ba3a467473eb23ff78c11006ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f209ea2ab83403e86a3e1c441629a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6386283411d440ffb512da6f71437c51",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0828b75bdcf34eb790fb6ffbd365a4f2"
          }
        },
        "ae497f7df0f3470c9625b77aa6b7312b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_03413a94384c47018a00e94a5e730008",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1821/0 [00:00&lt;00:00, 23260.74 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24cbde0d186442e9b8b71efad168dfa4"
          }
        },
        "6386283411d440ffb512da6f71437c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0828b75bdcf34eb790fb6ffbd365a4f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03413a94384c47018a00e94a5e730008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24cbde0d186442e9b8b71efad168dfa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chong-z/NLG-project/blob/master/CS269_NLG_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8npvE8zXZTCl"
      },
      "source": [
        "# CS269 NLG Project: Generating Semi-Restricted Natural Language Adversarial Examples\n",
        "Group Member: Chong Zhang (teammate dropped the class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29gLB7-fZemO"
      },
      "source": [
        "## Setup\n",
        "Install dependencies and clone the repo. May take a few minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1rXH4kSYLkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8340b081-af20-41f4-9e71-421f241491d4"
      },
      "source": [
        "!pip install pytorch-pretrained-bert==0.6.2 nlp torch nltk numpy tensorboardX pandas lm-scorer\n",
        "\n",
        "# lm-scorer installs a different version of transformers.\n",
        "!pip install transformers==3.0.2\n",
        "\n",
        "!git clone https://github.com/chong-z/NLG-project.git\n",
        "%cd NLG-project\n",
        "!sh dowloaddata.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert==0.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.7MB/s \n",
            "\u001b[?25hCollecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Collecting lm-scorer\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/89/d86ee877bfa51104b338a67413c76b6fde50a76c7b7e0c55c546effe97e9/lm_scorer-0.4.2-py3-none-any.whl\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.2) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/65/a573dd1cc6ca76815f358f9e5be449221f4017a87b2e6b5ea970df28f94b/boto3-1.16.33-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.2) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
            "Collecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 213kB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Collecting transformers<3.0.0,>=2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 61.2MB/s \n",
            "\u001b[?25hCollecting pip>=20.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/11/2dc62c5263d9eb322f2f028f7b56cd9d096bb8988fcf82d65fa2e4057afe/pip-20.3.1-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 35.0MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.33\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/3e/5f64c7b492312069520d219ed56ce6b3d1821dab8251c18d25728578b58e/botocore-1.19.33-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 48.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.2) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.2) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.0.0,>=2.9.0->lm-scorer) (20.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.0.0,>=2.9.0->lm-scorer) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.0.0,>=2.9.0->lm-scorer) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.0.0,>=2.9.0->lm-scorer) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=db5e1fe2b7d1fea83f35195b1531ab6b8f662548457c94507be076add77db1d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.33 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, pyarrow, xxhash, nlp, tensorboardX, sacremoses, sentencepiece, tokenizers, transformers, pip, lm-scorer\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed boto3-1.16.33 botocore-1.19.33 jmespath-0.10.0 lm-scorer-0.4.2 nlp-0.4.0 pip-20.3.1 pyarrow-2.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tensorboardX-2.1 tokenizers-0.7.0 transformers-2.11.0 xxhash-2.0.0\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.94)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (1.15.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 43.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.7.0\n",
            "    Uninstalling tokenizers-0.7.0:\n",
            "      Successfully uninstalled tokenizers-0.7.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 2.11.0\n",
            "    Uninstalling transformers-2.11.0:\n",
            "      Successfully uninstalled transformers-2.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lm-scorer 0.4.2 requires transformers<3.0.0,>=2.9.0, but you have transformers 3.0.2 which is incompatible.\u001b[0m\n",
            "Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Cloning into 'NLG-project'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Counting objects: 100% (205/205), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 205 (delta 107), reused 154 (delta 71), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (205/205), 67.56 MiB | 34.78 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n",
            "/content/NLG-project\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2020-12-09 21:56:24--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  2.94MB/s    in 12s     \n",
            "\n",
            "2020-12-09 21:56:37 (2.68 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVy8sLKdd3is"
      },
      "source": [
        "## Explore SST-2\n",
        "We focus the study on the SST-2 dataset. Here is a peek on the training examples.\n",
        "\n",
        "Label 0 and 1 denotes negative and positive sentiment, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930,
          "referenced_widgets": [
            "d05062687ece451c8df7bcfa3cd5357d",
            "5c9564c049124fb18cee3930ee1a7f5b",
            "8fe4b42bdce14da1ab34653e8d1280e5",
            "894912b59609403f97f8d2f447a6fbea",
            "8dda308c099542ff9ffc864fdcf0cc6b",
            "cf3af74a30894543845a28faab644db2",
            "9a55af8b5689417e9b6a8f949deddeea",
            "a28f0e2d14094b88b3eb1335aa90c58e",
            "96a8848b603e455c916d98908c7111fc",
            "3fd3335dc91f490387c2d7509cf39bf2",
            "1d3b26a4d36b4391890770d79727f34a",
            "1ec377b26cf545a2881819d026fc1404",
            "a14ff1ba53be4d3cb52e523b37e8b33d",
            "4a5152646f7b468ea08007226c932712",
            "d50c822d6b5741e3b52203a801037574",
            "f73729b95b234405afedf02feca1a4e1",
            "9a35673f5bd4479f9a1e95cefa0f18a3",
            "669d988f904c45058ed33142accc8446",
            "003319d5b2914f269931b1712a480e19",
            "018ee90f21df402daf4df1804e96d99a",
            "727bfa3f55b94664b0c75ab287d8fed7",
            "0b5f587e56b94de88e6a6b8d70956e8a",
            "5f8878f5dc974edfa18ec6ca0d9f036d",
            "6db472cda6a240bf854934374646c454",
            "995ebde83b644a938fd33f8a6ad67725",
            "a18ef9c312f64645b5bf9843c442fb16",
            "e725e420d9904a33a3d1a768e85b56e2",
            "4b03c24143084e2fb8a6543a00a7d27d",
            "9a2bceab5a6743248d0b4beed72f8324",
            "ff26af976166491094a11fc079613f97",
            "83d307d8091949438802573dc4895a5c",
            "921a45e4863b47f3b196662439916ec3",
            "6abca299ad4749d58fa054d824da4743",
            "c27e468ae9e74f198a490463333d90e3",
            "036a1feccbce4427b5c99ffa000a2f34",
            "f45cb0a7bc1649cca431e15c997b3fe1",
            "062451a616304eef8f3efdbbf0297f14",
            "0956b9ed53514274a56f12d58933c635",
            "2bdecdde19e846569afa3062cc569c7d",
            "e1ee47d52f8c462c8c73e2e908408c4c",
            "a184875193e645b3864ff224ec8a9425",
            "1c272ba3a467473eb23ff78c11006ba5",
            "6f209ea2ab83403e86a3e1c441629a9a",
            "ae497f7df0f3470c9625b77aa6b7312b",
            "6386283411d440ffb512da6f71437c51",
            "0828b75bdcf34eb790fb6ffbd365a4f2",
            "03413a94384c47018a00e94a5e730008",
            "24cbde0d186442e9b8b71efad168dfa4"
          ]
        },
        "id": "tq31xGKvd8HI",
        "outputId": "e30fa9cc-2e76-4a67-ee1a-6b6b5ea3ba08"
      },
      "source": [
        "import nlp\n",
        "import pandas as pd\n",
        "pd.options.display.min_rows = 20\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "sst2_data = nlp.load_dataset('glue', 'sst2')['train']\n",
        "df = pd.DataFrame(sst2_data)\n",
        "display(df)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d05062687ece451c8df7bcfa3cd5357d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28940.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96a8848b603e455c916d98908c7111fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=30329.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown sizetotal: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a35673f5bd4479f9a1e95cefa0f18a3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7439277.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "995ebde83b644a938fd33f8a6ad67725",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6abca299ad4749d58fa054d824da4743",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a184875193e645b3864ff224ec8a9425",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>contains no wit , only labored gags</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>that loves its characters and communicates something rather beautiful about human nature</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>remains utterly satisfied to remain the same throughout</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the filmmakers could dredge up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>that 's far too tragic to merit such superficial treatment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>of saucy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>a depressed fifteen-year-old 's suicidal poetry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>are more deeply thought through than in most ` right-thinking ' films</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67339</th>\n",
              "      <td>67339</td>\n",
              "      <td>1</td>\n",
              "      <td>works more often than it does n't .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67340</th>\n",
              "      <td>67340</td>\n",
              "      <td>1</td>\n",
              "      <td>at least passably</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67341</th>\n",
              "      <td>67341</td>\n",
              "      <td>0</td>\n",
              "      <td>i also believe that resident evil is not it .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67342</th>\n",
              "      <td>67342</td>\n",
              "      <td>0</td>\n",
              "      <td>seem to be in a contest to see who can out-bad-act the other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67343</th>\n",
              "      <td>67343</td>\n",
              "      <td>1</td>\n",
              "      <td>showing off his doctorate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67344</th>\n",
              "      <td>67344</td>\n",
              "      <td>1</td>\n",
              "      <td>a delightful comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67345</th>\n",
              "      <td>67345</td>\n",
              "      <td>0</td>\n",
              "      <td>anguish , anger and frustration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67346</th>\n",
              "      <td>67346</td>\n",
              "      <td>1</td>\n",
              "      <td>at achieving the modest , crowd-pleasing goals it sets for itself</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67347</th>\n",
              "      <td>67347</td>\n",
              "      <td>1</td>\n",
              "      <td>a patient viewer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67348</th>\n",
              "      <td>67348</td>\n",
              "      <td>0</td>\n",
              "      <td>this new jangle of noise , mayhem and stupidity must be a serious contender for the title .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67349 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         idx  ...                                                                                                                                               sentence\n",
              "0          0  ...                                                                                                           hide new secretions from the parental units \n",
              "1          1  ...                                                                                                                   contains no wit , only labored gags \n",
              "2          2  ...                                                              that loves its characters and communicates something rather beautiful about human nature \n",
              "3          3  ...                                                                                               remains utterly satisfied to remain the same throughout \n",
              "4          4  ...                                                                              on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
              "5          5  ...                                                                                            that 's far too tragic to merit such superficial treatment \n",
              "6          6  ...  demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \n",
              "7          7  ...                                                                                                                                              of saucy \n",
              "8          8  ...                                                                                                       a depressed fifteen-year-old 's suicidal poetry \n",
              "9          9  ...                                                                                 are more deeply thought through than in most ` right-thinking ' films \n",
              "...      ...  ...                                                                                                                                                    ...\n",
              "67339  67339  ...                                                                                                                   works more often than it does n't . \n",
              "67340  67340  ...                                                                                                                                     at least passably \n",
              "67341  67341  ...                                                                                                         i also believe that resident evil is not it . \n",
              "67342  67342  ...                                                                                          seem to be in a contest to see who can out-bad-act the other \n",
              "67343  67343  ...                                                                                                                             showing off his doctorate \n",
              "67344  67344  ...                                                                                                                                   a delightful comedy \n",
              "67345  67345  ...                                                                                                                       anguish , anger and frustration \n",
              "67346  67346  ...                                                                                     at achieving the modest , crowd-pleasing goals it sets for itself \n",
              "67347  67347  ...                                                                                                                                      a patient viewer \n",
              "67348  67348  ...                                                           this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \n",
              "\n",
              "[67349 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p_ntouCbV-X"
      },
      "source": [
        "## Run adversarial attacks\n",
        "`semi_attack.py` is the main attack script. It takes in a few parameters:\n",
        "\n",
        " - `-c models/sample-GRU/E9.pytorch`: Use our pre-trained GRU VAE for generating interpolations. Please refer to the next section on how to train your own VAE.\n",
        " - `--iter 2`: Use 2 iterations for the 'binary' search.\n",
        " - `--steps 10`: Sample 10 interpolations per iteration.\n",
        " - `--victim_model \"distilbert-base-uncased-finetuned-sst-2-english\"`: Use the pre-trained [\"distilbert-base-uncased-finetuned-sst-2-english\"](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model from HuggingFace. Other models such as \"textattack/roberta-base-SST-2\" can also be used.\n",
        " - `--victim_sentence \"i study at ucla\"`: The victim sentence under attack. Our goal is to find an adversarial sentence similar but with different prediction than the victim sentence.\n",
        " - `--reference_sentence \"i finished my final exam at ucla\"`: The reference sentence providing the hint for the desired style.\n",
        " - `--most_similar` vs. `--ppl_use`: Finds the best adversarial example based on VAE latent distance or the Universal Sentence Encoder consine similarity.\n",
        " - Please refer to the script for additional parameters.\n",
        "\n",
        " \n",
        " ### Example 1\n",
        "\n",
        " In this example, the VAE only generates movie reviews despite the input (as expected). However, they do share some similarities with the victim and the reference sentence. For instance, they usually starts with an \"i\" and have the similar length.\n",
        "\n",
        " The final output is:\n",
        " ```\n",
        " -------Attack Result-------\n",
        "Victim Sentence: i study at ucla pred:0.8251549005508423\n",
        "Best Adv Sentence: this woefully hackneyed movie with flailing bodily movements <eos> pred:0.00024062106967903674\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LAOJJMrcUOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d51df7-a414-4101-ba68-1e35caa2205b"
      },
      "source": [
        "!python semi_attack.py -c models/sample-GRU/E9.pytorch --iter 2 --steps 10 --rseed 7 --most_similar -v \\\n",
        "  --victim_model \"distilbert-base-uncased-finetuned-sst-2-english\" \\\n",
        "  --victim_sentence \"i study at ucla\" \\\n",
        "  --reference_sentence \"i finished my final exam at ucla\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-09 21:56:50.059864: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Loading the VAE model...\n",
            "VALID preprocessed file not found at data/ptb.valid.json. Creating new.\n",
            "Model loaded from models/sample-GRU/E9.pytorch\n",
            "Loading the victim model from HuggingFace...\n",
            "Downloading: 100% 629/629 [00:00<00:00, 559kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.17MB/s]\n",
            "Downloading: 100% 268M/268M [00:09<00:00, 27.3MB/s]\n",
            "\n",
            "-------Debug Mode-------\n",
            "\n",
            "-------Initial Inputs-------\n",
            "Victim Sentence: i study at ucla pred:0.8251549005508423 PPL:0 USE:0.00\n",
            "Reference Sentence: i finished my final exam at ucla pred:0.07655958086252213 PPL:0 USE:0.00\n",
            "\n",
            "-------ITERATION 0-------\n",
            "Best Adv Sentence: i finished my final exam at ucla pred:0.07655958086252213\n",
            "-------PREDICTIONS-------\n",
            "Pred & Sentence & PPL & USE \\\\\n",
            "0.825 & i study at ucla & 0 & 0.00 \\\\\n",
            "0.000 & a movie filled with unlikable , spiteful idiots  & 0 & 0.00 \\\\\n",
            "1.000 & a movie that will enthrall the whole family  & 0 & 0.00 \\\\\n",
            "0.003 & i ' ve seen before i saw this movie ,  & 0 & 0.00 \\\\\n",
            "0.000 & i saw this movie , i think it ' s just another crime movie  & 0 & 0.00 \\\\\n",
            "0.001 & i saw this movie with a taste for exaggeration  & 0 & 0.00 \\\\\n",
            "0.999 & i saw this movie with a taste  & 0 & 0.00 \\\\\n",
            "0.999 & i saw this movie with it  & 0 & 0.00 \\\\\n",
            "0.999 & i saw this movie  & 0 & 0.00 \\\\\n",
            "0.077 & i finished my final exam at ucla & 0 & 0.00 \\\\\n",
            "\n",
            "-------ITERATION 1-------\n",
            "Best Adv Sentence: a movie filled with unlikable , spiteful idiots  pred:0.07655958086252213\n",
            "-------PREDICTIONS-------\n",
            "Pred & Sentence & PPL & USE \\\\\n",
            "0.825 & i study at ucla & 0 & 0.00 \\\\\n",
            "0.000 & this woefully hackneyed movie with flailing bodily movements  & 0 & 0.00 \\\\\n",
            "1.000 & a delicious crime drama  & 0 & 0.00 \\\\\n",
            "0.000 & a movie filled with unlikable , spiteful idiots  & 0 & 0.00 \\\\\n",
            "-------Attack Result-------\n",
            "Victim Sentence: i study at ucla pred:0.8251549005508423 PPL:0 USE:0.00\n",
            "Best Adv Sentence: this woefully hackneyed movie with flailing bodily movements  pred:0.00020179840794298798 PPL:0 USE:0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHNdnQ6PHZ0b"
      },
      "source": [
        "### Example 2\n",
        "In this example, we pass in appropriate movie reviews to the script. We find adversarial examples in 2 iterations:\n",
        "\n",
        "1. In the first iteration, our method finds the sentence `an intriguing story , but ultimately purposeless , ...`, which has a different prediction than the victim example. We use it ass the reference example for the next iteration.\n",
        "2. After the second iteration, our method outputs the best adversarial example `the story is bogus and directed by joel ...`, which also has a different prediction but even closer to the victim sentence.\n",
        "\n",
        "The final output is:\n",
        "```\n",
        "-------Attack Result-------\n",
        "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688\n",
        "Best Adv Sentence: the story is bogus and directed by joel schumacher and a half dozen young men who has been overexposed , redolent of the plot device <eos> pred:0.0007369006052613258\n",
        "```\n",
        "\n",
        "Please note that we may need additional iterations to find a more similar adversarial example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBWAP3MjZKm0",
        "outputId": "838e5abb-bfcf-4130-8cc1-39fda94c227c"
      },
      "source": [
        "!python semi_attack.py -c models/sample-GRU/E9.pytorch --iter 2 --steps 10 --rseed 3 --most_similar -v \\\n",
        "  --victim_model \"distilbert-base-uncased-finetuned-sst-2-english\" \\\n",
        "  --victim_sentence \"a strangely compelling and brilliantly acted psychological drama .\" \\\n",
        "  --reference_sentence \"an absurdist sitcom about alienation , separation and loss .\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-09 22:05:33.126758: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Loading the VAE model...\n",
            "Model loaded from models/sample-GRU/E9.pytorch\n",
            "Loading the victim model from HuggingFace...\n",
            "\n",
            "-------Debug Mode-------\n",
            "\n",
            "-------Initial Inputs-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688 PPL:0 USE:0.00\n",
            "Reference Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968 PPL:0 USE:0.00\n",
            "\n",
            "-------ITERATION 0-------\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "Pred & Sentence & PPL & USE \\\\\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . & 0 & 0.00 \\\\\n",
            "0.999 & a quietly introspective portrait of pure misogynist evil  & 0 & 0.00 \\\\\n",
            "1.000 & a quietly moving portrait of an intelligent screenplay  & 0 & 0.00 \\\\\n",
            "0.999 & an intriguing story , but ultimately purposeless and satisfying heroine  & 0 & 0.00 \\\\\n",
            "0.001 & an intriguing story , but ultimately purposeless , and ultimately empty examination of the modern rut of the entire scenario .  & 0 & 0.00 \\\\\n",
            "1.000 & an intriguing story , but the story simply because it is a refreshingly forthright one .  & 0 & 0.00 \\\\\n",
            "0.003 & an absurdist sitcom about alienation , separation and loss . & 0 & 0.00 \\\\\n",
            "\n",
            "-------ITERATION 1-------\n",
            "Best Adv Sentence: an intriguing story , but ultimately purposeless , and ultimately empty examination of the modern rut of the entire scenario .  pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "Pred & Sentence & PPL & USE \\\\\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . & 0 & 0.00 \\\\\n",
            "1.000 & a delicious crime drama that revives the free-wheeling noir spirit of its spirit with its own cuteness  & 0 & 0.00 \\\\\n",
            "1.000 & a delicious crime drama that manages to invest its target audience talked the experience of its own ironic implications .  & 0 & 0.00 \\\\\n",
            "1.000 & a delicious crime drama that manages to invest its target audience talked the road warrior .  & 0 & 0.00 \\\\\n",
            "1.000 & a delicious crime drama that manages to invest its target audience talked the buoyant energy of surprise .  & 0 & 0.00 \\\\\n",
            "0.952 & a quietly introspective portrait of a subculture hell-bent on the emptiness of creating a screenplay  & 0 & 0.00 \\\\\n",
            "1.000 & the premise of a handsome and well-made entertainment  & 0 & 0.00 \\\\\n",
            "0.002 & the story is bogus and directed by joel schumacher and a half dozen young men who knows how to tell .  & 0 & 0.00 \\\\\n",
            "0.021 & the awkwardly paced and the euphoria of growing up with its exquisite acting , inventive screenplay and listless direction .  & 0 & 0.00 \\\\\n",
            "0.374 & the awkwardly paced and the euphoria of the pool with its exquisite acting , inventive screenplay and listless direction .  & 0 & 0.00 \\\\\n",
            "0.001 & an intriguing story , but ultimately purposeless , and ultimately empty examination of the modern rut of the entire scenario .  & 0 & 0.00 \\\\\n",
            "-------Attack Result-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688 PPL:0 USE:0.00\n",
            "Best Adv Sentence: the story is bogus and directed by joel schumacher and a half dozen young men who knows how to tell .  pred:0.001792142167687416 PPL:0 USE:0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdAYGFS1gS6C"
      },
      "source": [
        "### Example 3\n",
        "We also provide a pre-trained LSTM-based VAE model. To use we only need to replace the corresponding parameters with `-c models/sample-LSTM/E9.pytorch --rnn_type lstm`.\n",
        "\n",
        "For this example we replaced the parameter `--most_similar` with `--ppl_use`, which finds the best adversarial example according to their Universal Sentence Encoder cosine similarity.\n",
        "\n",
        "Note: It runs significant slower due to the Universal Sentence Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzV4hauQgSLK",
        "outputId": "3c2caa0e-4d65-49d7-c6bf-4a5b0c8f7575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python semi_attack.py -c models/sample-LSTM/E9.pytorch --rnn_type lstm --iter 2 --steps 10 --rseed 3 --ppl_use -v \\\n",
        "  --victim_model \"distilbert-base-uncased-finetuned-sst-2-english\" \\\n",
        "  --victim_sentence \"a strangely compelling and brilliantly acted psychological drama .\" \\\n",
        "  --reference_sentence \"an absurdist sitcom about alienation , separation and loss .\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-09 22:06:16.076536: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Loading GPT-2...\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading UniversalSentenceEncoder...\n",
            "2020-12-09 22:06:29.561563: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-12-09 22:06:29.561765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.562311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-12-09 22:06:29.562352: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-09 22:06:29.563923: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-12-09 22:06:29.565537: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-12-09 22:06:29.565884: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-12-09 22:06:29.567410: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-12-09 22:06:29.568146: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-12-09 22:06:29.571096: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-12-09 22:06:29.571223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.571751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.572219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "2020-12-09 22:06:29.827492: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-12-09 22:06:29.827716: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ae44000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-09 22:06:29.827748: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-12-09 22:06:29.829125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.829747: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ae441c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-09 22:06:29.829771: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-12-09 22:06:29.829944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.830480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-12-09 22:06:29.830517: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-09 22:06:29.830570: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-12-09 22:06:29.830595: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-12-09 22:06:29.830617: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-12-09 22:06:29.830643: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-12-09 22:06:29.830666: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-12-09 22:06:29.830690: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-12-09 22:06:29.830784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.831337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.831788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-12-09 22:06:29.831890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-09 22:06:29.831952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-12-09 22:06:29.831968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-12-09 22:06:29.831980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-12-09 22:06:29.832156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.832700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:06:29.833208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12667 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-12-09 22:06:30.616429: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "2020-12-09 22:06:30.632939: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "2020-12-09 22:06:30.648898: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "2020-12-09 22:06:30.666840: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "2020-12-09 22:06:30.683565: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "Loading the VAE model...\n",
            "Model loaded from models/sample-LSTM/E9.pytorch\n",
            "Loading the victim model from HuggingFace...\n",
            "\n",
            "-------Debug Mode-------\n",
            "2020-12-09 22:06:38.166956: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "\n",
            "-------Initial Inputs-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688 PPL:60 USE:1.00\n",
            "Reference Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968 PPL:68 USE:0.37\n",
            "\n",
            "-------ITERATION 0-------\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "Pred & Sentence & PPL & USE \\\\\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . & 60 & 1.00 \\\\\n",
            "0.002 & , it ' s a bargain-basement european pickup .  & 79 & -0.02 \\\\\n",
            "0.002 & the film is a subzero version of monsters , inc .  & 85 & 0.12 \\\\\n",
            "0.003 & an absurdist sitcom about alienation , separation and loss . & 68 & 0.37 \\\\\n",
            "\n",
            "-------ITERATION 1-------\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "Pred & Sentence & PPL & USE \\\\\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . & 60 & 1.00 \\\\\n",
            "0.002 & , it ' s a bargain-basement european pickup .  & 79 & -0.02 \\\\\n",
            "1.000 & a fascinating curiosity piece of filmmaking .  & 62 & 0.44 \\\\\n",
            "1.000 & a fascinating curiosity piece of filmmaking with a kiss  & 78 & 0.44 \\\\\n",
            "1.000 & a fascinating curiosity piece of filmmaking with a kiss of imagination and  & 92 & 0.50 \\\\\n",
            "0.003 & an absurdist sitcom about alienation , separation and loss . & 68 & 0.37 \\\\\n",
            "-------Attack Result-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688 PPL:60 USE:1.00\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968 PPL:68 USE:0.37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dd_fauWgppr"
      },
      "source": [
        "## Evaluation\n",
        "We provide the script to reproduce the results in our report. The script attacks 100 examples from the validation split, and finds reference sentences from the training set automatically.\n",
        "For each attack it runs 20 iterations with 20 interpolations per examples, and report the best adversarial example it could find.\n",
        "\n",
        "Note: The script may take hours to run depending on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kbbWBqxg_5K",
        "outputId": "75bc010d-5c47-4c32-b796-7b38b4045aba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python semi_attack.py -c models/sample-LSTM/E9.pytorch --rnn_type lstm --iter 20 --steps 20 --rseed 3 --n_attacks 20 --ppl_use --n_eval 100 -v --victim_model \"distilbert-base-uncased-finetuned-sst-2-english\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-09 21:59:27.273052: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Loading GPT-2...\n",
            "Downloading: 100% 665/665 [00:00<00:00, 579kB/s]\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.65MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.57MB/s]\n",
            "Downloading: 100% 548M/548M [00:29<00:00, 18.5MB/s]\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading UniversalSentenceEncoder...\n",
            "2020-12-09 22:00:12.363967: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-12-09 22:00:12.364358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:12.364920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-12-09 22:00:12.364962: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-09 22:00:12.623223: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-12-09 22:00:12.742366: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-12-09 22:00:12.771118: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-12-09 22:00:13.030793: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-12-09 22:00:13.071221: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-12-09 22:00:13.565937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-12-09 22:00:13.566208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:13.566864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:13.567354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
            "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
            "2020-12-09 22:00:25.277116: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-12-09 22:00:25.280654: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1613c000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-09 22:00:25.280711: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-12-09 22:00:25.290343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:25.296537: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1613c1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-09 22:00:25.296577: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-12-09 22:00:25.296851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:25.297755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-12-09 22:00:25.297828: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-09 22:00:25.297933: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-12-09 22:00:25.297975: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-12-09 22:00:25.298012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-12-09 22:00:25.298050: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-12-09 22:00:25.298092: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-12-09 22:00:25.298125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-12-09 22:00:25.298246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:25.298849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:25.299436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-12-09 22:00:25.303177: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-09 22:00:25.303297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-12-09 22:00:25.303320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-12-09 22:00:25.303334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-12-09 22:00:25.306932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:25.307596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 22:00:25.308152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12667 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-12-09 22:00:26.258871: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "2020-12-09 22:00:26.274724: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "2020-12-09 22:00:26.290066: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "2020-12-09 22:00:26.306756: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "2020-12-09 22:00:26.323013: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 34133760 exceeds 10% of free system memory.\n",
            "Loading the VAE model...\n",
            "Model loaded from models/sample-LSTM/E9.pytorch\n",
            "Loading the victim model from HuggingFace...\n",
            "\n",
            "-------Evaluation Mode-------\n",
            "\n",
            "-------Start Evaluation 0/100-------\n",
            "\n",
            "-------Running 20 Attacks-------\n",
            "sentences = ['sand to the fierce grandeur of its sweeping battle scenes ', 'finds warmth ', \"of the double-cross that made mamet 's `` house of games '' and last fall 's `` heist '' so much fun \", 'impressive for the sights and sounds of the wondrous beats ', 'makes us believe she is kahlo ', 'cinematic knack ', 'wins still ', 'follows its standard formula in this animated adventure ', \"combines the enigmatic features of ` memento ' \", 'charming but slight ', 'an interesting movie ', 'realism , crisp storytelling and ', 'bucked the odds to emerge as an exquisite motion picture in its own right ', 'captures all the longing , anguish and ache , the confusing sexual messages and the wish to be a part of that elusive adult world ', 'the action as gripping ', 'film buzz and whir ', 'with passion and attitude ', 'is one adapted - from-television movie that actually looks as if it belongs on the big screen ', 'a hallucinatory dreamscape ', 'crazy as hell marks an encouraging new direction for la salle . ']\n",
            "2020-12-09 22:05:04.510453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Traceback (most recent call last):\n",
            "  File \"semi_attack.py\", line 310, in <module>\n",
            "    main(args)\n",
            "  File \"semi_attack.py\", line 266, in main\n",
            "    do_evaluation(vae, victim_model, ppl, use, args)\n",
            "  File \"semi_attack.py\", line 209, in do_evaluation\n",
            "    adv = do_n_attacks(vae, vs, victim_model, ppl, use, args)\n",
            "  File \"semi_attack.py\", line 187, in do_n_attacks\n",
            "    adv = do_one_attack(vae, victim_sentence, s, victim_model, ppl, use, args)\n",
            "  File \"semi_attack.py\", line 128, in do_one_attack\n",
            "    best_use = use(start_sentence, end_sentence)\n",
            "  File \"semi_attack.py\", line 252, in use\n",
            "    return u.cos_sim(s1, s2)\n",
            "  File \"/content/NLG-project/universal_sentence_encoder.py\", line 15, in cos_sim\n",
            "    e1, e2 = self.model([s1, s2]).numpy()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 509, in _call_attribute\n",
            "    return instance.__call__(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\n",
            "    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\n",
            "    cancellation_manager=cancellation_manager)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\n",
            "    ctx, args, cancellation_manager=cancellation_manager))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 550, in call\n",
            "    ctx=ctx)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
            "    inputs, attrs, num_outputs)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nASttNMrKBmD"
      },
      "source": [
        "## Train VAE\n",
        "Train a LSTM-based VAE with 10 epoches and default settings, may take half an hour to run. Please refer to `train.py` for additional parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQKOVHcTKD5W",
        "outputId": "3c528d79-e64e-466f-9169-cd9665f4ebe4"
      },
      "source": [
        "!python train.py --data_dir data --epochs 10 --rnn_type lstm -tb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 478750579/478750579 [00:10<00:00, 47853190.10B/s]\n",
            "100% 656/656 [00:00<00:00, 626614.31B/s]\n",
            "100% 815973/815973 [00:00<00:00, 28416838.83B/s]\n",
            "100% 458495/458495 [00:00<00:00, 18688156.93B/s]\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
            "TRAIN preprocessed file not found at data/ptb.train.json. Creating new.\n",
            "2020-12-07 21:43:48.096056: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Vocablurary of 14221 keys created.\n",
            "SentenceVAE(\n",
            "  (embedding): Embedding(14221, 300)\n",
            "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
            "  (encoder_rnn): LSTM(300, 256, batch_first=True)\n",
            "  (decoder_rnn): LSTM(300, 256, batch_first=True)\n",
            "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
            "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
            "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
            "  (outputs2vocab): Linear(in_features=256, out_features=14221, bias=True)\n",
            ")\n",
            "TRAIN Batch 0000/2104, Loss  117.1735, NLL-Loss  117.1733, KL-Loss    0.0940, KL-Weight  0.002, Perp-loss    0.0000, Perp-weight 519.013\n",
            "TRAIN Batch 0050/2104, Loss   56.2679, NLL-Loss   56.1980, KL-Loss   32.0163, KL-Weight  0.002, Perp-loss    0.0000, Perp-weight 458.145\n",
            "TRAIN Batch 0100/2104, Loss   68.9362, NLL-Loss   68.8908, KL-Loss   18.3773, KL-Weight  0.002, Perp-loss    0.0000, Perp-weight 404.429\n",
            "TRAIN Batch 0150/2104, Loss   55.6245, NLL-Loss   55.5623, KL-Loss   22.2133, KL-Weight  0.003, Perp-loss    0.0000, Perp-weight 357.025\n",
            "TRAIN Batch 0200/2104, Loss   69.1101, NLL-Loss   69.0164, KL-Loss   29.5419, KL-Weight  0.003, Perp-loss    0.0000, Perp-weight 315.191\n",
            "TRAIN Batch 0250/2104, Loss   57.9804, NLL-Loss   57.8665, KL-Loss   31.6928, KL-Weight  0.004, Perp-loss    0.0000, Perp-weight 278.272\n",
            "TRAIN Batch 0300/2104, Loss   75.7385, NLL-Loss   75.5599, KL-Loss   43.8708, KL-Weight  0.004, Perp-loss    0.0000, Perp-weight 245.692\n",
            "TRAIN Batch 0350/2104, Loss   57.6136, NLL-Loss   57.4446, KL-Loss   36.6496, KL-Weight  0.005, Perp-loss    0.0000, Perp-weight 216.940\n",
            "TRAIN Batch 0400/2104, Loss   71.8752, NLL-Loss   71.6358, KL-Loss   45.8511, KL-Weight  0.005, Perp-loss    0.0000, Perp-weight 191.566\n",
            "TRAIN Batch 0450/2104, Loss   46.8487, NLL-Loss   46.6387, KL-Loss   35.5357, KL-Weight  0.006, Perp-loss    0.0000, Perp-weight 169.174\n",
            "TRAIN Batch 0500/2104, Loss   68.1306, NLL-Loss   67.8702, KL-Loss   38.9052, KL-Weight  0.007, Perp-loss    0.0000, Perp-weight 149.413\n",
            "TRAIN Batch 0550/2104, Loss   58.0463, NLL-Loss   57.7946, KL-Loss   33.2193, KL-Weight  0.008, Perp-loss    0.0000, Perp-weight 131.974\n",
            "TRAIN Batch 0600/2104, Loss   65.3999, NLL-Loss   65.0730, KL-Loss   38.1189, KL-Weight  0.009, Perp-loss    0.0000, Perp-weight 116.584\n",
            "TRAIN Batch 0650/2104, Loss   51.6520, NLL-Loss   51.3406, KL-Loss   32.0679, KL-Weight  0.010, Perp-loss    0.0000, Perp-weight 103.003\n",
            "TRAIN Batch 0700/2104, Loss   49.7676, NLL-Loss   49.4129, KL-Loss   32.2837, KL-Weight  0.011, Perp-loss    0.0000, Perp-weight 91.017\n",
            "TRAIN Batch 0750/2104, Loss   63.6862, NLL-Loss   63.2773, KL-Loss   32.8918, KL-Weight  0.012, Perp-loss    0.0000, Perp-weight 80.440\n",
            "TRAIN Batch 0800/2104, Loss   45.4556, NLL-Loss   45.0320, KL-Loss   30.1187, KL-Weight  0.014, Perp-loss    0.0000, Perp-weight 71.105\n",
            "TRAIN Batch 0850/2104, Loss   56.8253, NLL-Loss   56.3737, KL-Loss   28.3909, KL-Weight  0.016, Perp-loss    0.0000, Perp-weight 62.868\n",
            "TRAIN Batch 0900/2104, Loss   47.0390, NLL-Loss   46.5338, KL-Loss   28.0899, KL-Weight  0.018, Perp-loss    0.0000, Perp-weight 55.598\n",
            "TRAIN Batch 0950/2104, Loss   58.3619, NLL-Loss   57.7779, KL-Loss   28.7269, KL-Weight  0.020, Perp-loss    0.0000, Perp-weight 49.183\n",
            "TRAIN Batch 1000/2104, Loss   60.9053, NLL-Loss   60.2478, KL-Loss   28.6147, KL-Weight  0.023, Perp-loss    0.0000, Perp-weight 43.521\n",
            "TRAIN Batch 1050/2104, Loss   62.4329, NLL-Loss   61.6506, KL-Loss   30.1382, KL-Weight  0.026, Perp-loss    0.0000, Perp-weight 38.525\n",
            "TRAIN Batch 1100/2104, Loss   62.9773, NLL-Loss   62.1483, KL-Loss   28.2801, KL-Weight  0.029, Perp-loss    0.0000, Perp-weight 34.115\n",
            "TRAIN Batch 1150/2104, Loss   49.5661, NLL-Loss   48.7420, KL-Loss   24.9100, KL-Weight  0.033, Perp-loss    0.0000, Perp-weight 30.224\n",
            "TRAIN Batch 1200/2104, Loss   53.4327, NLL-Loss   52.4484, KL-Loss   26.3684, KL-Weight  0.037, Perp-loss    0.0000, Perp-weight 26.790\n",
            "TRAIN Batch 1250/2104, Loss   41.5582, NLL-Loss   40.6225, KL-Loss   22.2333, KL-Weight  0.042, Perp-loss    0.0000, Perp-weight 23.760\n",
            "TRAIN Batch 1300/2104, Loss   52.4263, NLL-Loss   51.3708, KL-Loss   22.2556, KL-Weight  0.047, Perp-loss    0.0000, Perp-weight 21.086\n",
            "TRAIN Batch 1350/2104, Loss   60.0992, NLL-Loss   58.7947, KL-Loss   24.4270, KL-Weight  0.053, Perp-loss    0.0000, Perp-weight 18.725\n",
            "TRAIN Batch 1400/2104, Loss   48.2092, NLL-Loss   46.9877, KL-Loss   20.3284, KL-Weight  0.060, Perp-loss    0.0000, Perp-weight 16.643\n",
            "TRAIN Batch 1450/2104, Loss   53.4667, NLL-Loss   52.1071, KL-Loss   20.1284, KL-Weight  0.068, Perp-loss    0.0000, Perp-weight 14.805\n",
            "TRAIN Batch 1500/2104, Loss   46.1805, NLL-Loss   44.8617, KL-Loss   17.3850, KL-Weight  0.076, Perp-loss    0.0000, Perp-weight 13.182\n",
            "TRAIN Batch 1550/2104, Loss   58.9334, NLL-Loss   57.3340, KL-Loss   18.7945, KL-Weight  0.085, Perp-loss    0.0000, Perp-weight 11.751\n",
            "TRAIN Batch 1600/2104, Loss   58.8384, NLL-Loss   57.0783, KL-Loss   18.4597, KL-Weight  0.095, Perp-loss    0.0000, Perp-weight 10.488\n",
            "TRAIN Batch 1650/2104, Loss   56.7078, NLL-Loss   54.7654, KL-Loss   18.2058, KL-Weight  0.107, Perp-loss    0.0000, Perp-weight  9.373\n",
            "TRAIN Batch 1700/2104, Loss   48.5429, NLL-Loss   46.7235, KL-Loss   15.2636, KL-Weight  0.119, Perp-loss    0.0000, Perp-weight  8.389\n",
            "TRAIN Batch 1750/2104, Loss   46.4791, NLL-Loss   44.4535, KL-Loss   15.2339, KL-Weight  0.133, Perp-loss    0.0000, Perp-weight  7.521\n",
            "TRAIN Batch 1800/2104, Loss   56.6764, NLL-Loss   54.4957, KL-Loss   14.7296, KL-Weight  0.148, Perp-loss    0.0000, Perp-weight  6.755\n",
            "TRAIN Batch 1850/2104, Loss   64.6702, NLL-Loss   62.1939, KL-Loss   15.0516, KL-Weight  0.165, Perp-loss    0.0000, Perp-weight  6.078\n",
            "TRAIN Batch 1900/2104, Loss   49.2279, NLL-Loss   47.0085, KL-Loss   12.1658, KL-Weight  0.182, Perp-loss    0.0000, Perp-weight  5.482\n",
            "TRAIN Batch 1950/2104, Loss   40.1334, NLL-Loss   38.0165, KL-Loss   10.4896, KL-Weight  0.202, Perp-loss    0.0000, Perp-weight  4.955\n",
            "TRAIN Batch 2000/2104, Loss   51.2603, NLL-Loss   48.3996, KL-Loss   12.8455, KL-Weight  0.223, Perp-loss    0.0000, Perp-weight  4.490\n",
            "TRAIN Batch 2050/2104, Loss   48.7541, NLL-Loss   46.1086, KL-Loss   10.7944, KL-Weight  0.245, Perp-loss    0.0000, Perp-weight  4.080\n",
            "TRAIN Batch 2100/2104, Loss   45.3760, NLL-Loss   42.5630, KL-Loss   10.4595, KL-Weight  0.269, Perp-loss    0.0000, Perp-weight  3.718\n",
            "TRAIN Batch 2104/2104, Loss   70.9126, NLL-Loss   67.6704, KL-Loss   11.9678, KL-Weight  0.271, Perp-loss    0.0000, Perp-weight  3.691\n",
            "TRAIN Epoch 00/10, Mean ELBO   57.6613\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E0.pytorch\n",
            "VALID Batch 0000/27, Loss  107.2146, NLL-Loss  103.0076, KL-Loss   15.5010, KL-Weight  0.271, Perp-loss    0.0000, Perp-weight  3.685\n",
            "VALID Batch 0027/27, Loss   86.8678, NLL-Loss   82.8126, KL-Loss   14.9414, KL-Weight  0.271, Perp-loss    0.0000, Perp-weight  3.685\n",
            "VALID Epoch 00/10, Mean ELBO  111.7014\n",
            "TRAIN Batch 0000/2104, Loss   49.6764, NLL-Loss   46.8638, KL-Loss   10.3628, KL-Weight  0.271, Perp-loss    0.0000, Perp-weight  3.685\n",
            "TRAIN Batch 0050/2104, Loss   61.0010, NLL-Loss   57.4840, KL-Loss   11.8493, KL-Weight  0.297, Perp-loss    0.0000, Perp-weight  3.369\n",
            "TRAIN Batch 0100/2104, Loss   49.0961, NLL-Loss   45.7749, KL-Loss   10.2647, KL-Weight  0.324, Perp-loss    0.0000, Perp-weight  3.091\n",
            "TRAIN Batch 0150/2104, Loss   49.8472, NLL-Loss   46.7330, KL-Loss    8.8601, KL-Weight  0.351, Perp-loss    0.0000, Perp-weight  2.845\n",
            "TRAIN Batch 0200/2104, Loss   69.8218, NLL-Loss   65.8480, KL-Loss   10.4443, KL-Weight  0.380, Perp-loss    0.0000, Perp-weight  2.628\n",
            "TRAIN Batch 0250/2104, Loss   53.6047, NLL-Loss   50.2948, KL-Loss    8.0660, KL-Weight  0.410, Perp-loss    0.0000, Perp-weight  2.437\n",
            "TRAIN Batch 0300/2104, Loss   39.2068, NLL-Loss   36.0041, KL-Loss    7.2641, KL-Weight  0.441, Perp-loss    0.0000, Perp-weight  2.268\n",
            "TRAIN Batch 0350/2104, Loss   46.6639, NLL-Loss   43.3560, KL-Loss    7.0097, KL-Weight  0.472, Perp-loss    0.0000, Perp-weight  2.119\n",
            "TRAIN Batch 0400/2104, Loss   43.0798, NLL-Loss   39.7640, KL-Loss    6.5905, KL-Weight  0.503, Perp-loss    0.0000, Perp-weight  1.988\n",
            "TRAIN Batch 0450/2104, Loss   54.9723, NLL-Loss   51.0667, KL-Loss    7.3095, KL-Weight  0.534, Perp-loss    0.0000, Perp-weight  1.872\n",
            "TRAIN Batch 0500/2104, Loss   53.0121, NLL-Loss   49.1440, KL-Loss    6.8432, KL-Weight  0.565, Perp-loss    0.0000, Perp-weight  1.769\n",
            "TRAIN Batch 0550/2104, Loss   52.0545, NLL-Loss   48.6347, KL-Loss    5.7410, KL-Weight  0.596, Perp-loss    0.0000, Perp-weight  1.679\n",
            "TRAIN Batch 0600/2104, Loss   43.2328, NLL-Loss   40.0507, KL-Loss    5.0882, KL-Weight  0.625, Perp-loss    0.0000, Perp-weight  1.599\n",
            "TRAIN Batch 0650/2104, Loss   53.1544, NLL-Loss   49.7925, KL-Loss    5.1391, KL-Weight  0.654, Perp-loss    0.0000, Perp-weight  1.529\n",
            "TRAIN Batch 0700/2104, Loss   62.7326, NLL-Loss   59.0061, KL-Loss    5.4649, KL-Weight  0.682, Perp-loss    0.0000, Perp-weight  1.466\n",
            "TRAIN Batch 0750/2104, Loss   60.8005, NLL-Loss   57.0257, KL-Loss    5.3288, KL-Weight  0.708, Perp-loss    0.0000, Perp-weight  1.412\n",
            "TRAIN Batch 0800/2104, Loss   66.6938, NLL-Loss   62.4966, KL-Loss    5.7221, KL-Weight  0.734, Perp-loss    0.0000, Perp-weight  1.363\n",
            "TRAIN Batch 0850/2104, Loss   52.5416, NLL-Loss   49.1486, KL-Loss    4.4809, KL-Weight  0.757, Perp-loss    0.0000, Perp-weight  1.321\n",
            "TRAIN Batch 0900/2104, Loss   45.5794, NLL-Loss   41.9929, KL-Loss    4.6013, KL-Weight  0.779, Perp-loss    0.0000, Perp-weight  1.283\n",
            "TRAIN Batch 0950/2104, Loss   38.0596, NLL-Loss   34.7148, KL-Loss    4.1800, KL-Weight  0.800, Perp-loss    0.0000, Perp-weight  1.250\n",
            "TRAIN Batch 1000/2104, Loss   44.6134, NLL-Loss   41.2222, KL-Loss    4.1385, KL-Weight  0.819, Perp-loss    0.0000, Perp-weight  1.220\n",
            "TRAIN Batch 1050/2104, Loss   53.6806, NLL-Loss   50.3454, KL-Loss    3.9838, KL-Weight  0.837, Perp-loss    0.0000, Perp-weight  1.194\n",
            "TRAIN Batch 1100/2104, Loss   48.7274, NLL-Loss   45.3318, KL-Loss    3.9784, KL-Weight  0.854, Perp-loss    0.0000, Perp-weight  1.172\n",
            "TRAIN Batch 1150/2104, Loss   41.0311, NLL-Loss   38.2327, KL-Loss    3.2222, KL-Weight  0.868, Perp-loss    0.0000, Perp-weight  1.151\n",
            "TRAIN Batch 1200/2104, Loss   48.6939, NLL-Loss   45.4282, KL-Loss    3.7022, KL-Weight  0.882, Perp-loss    0.0000, Perp-weight  1.134\n",
            "TRAIN Batch 1250/2104, Loss   56.3934, NLL-Loss   53.1286, KL-Loss    3.6499, KL-Weight  0.894, Perp-loss    0.0000, Perp-weight  1.118\n",
            "TRAIN Batch 1300/2104, Loss   55.4413, NLL-Loss   52.1728, KL-Loss    3.6087, KL-Weight  0.906, Perp-loss    0.0000, Perp-weight  1.104\n",
            "TRAIN Batch 1350/2104, Loss   52.3609, NLL-Loss   49.1093, KL-Loss    3.5503, KL-Weight  0.916, Perp-loss    0.0000, Perp-weight  1.092\n",
            "TRAIN Batch 1400/2104, Loss   41.9588, NLL-Loss   39.3583, KL-Loss    2.8113, KL-Weight  0.925, Perp-loss    0.0000, Perp-weight  1.081\n",
            "TRAIN Batch 1450/2104, Loss   53.7822, NLL-Loss   50.4770, KL-Loss    3.5416, KL-Weight  0.933, Perp-loss    0.0000, Perp-weight  1.072\n",
            "TRAIN Batch 1500/2104, Loss   54.7146, NLL-Loss   51.6012, KL-Loss    3.3099, KL-Weight  0.941, Perp-loss    0.0000, Perp-weight  1.063\n",
            "TRAIN Batch 1550/2104, Loss   40.5940, NLL-Loss   38.0185, KL-Loss    2.7190, KL-Weight  0.947, Perp-loss    0.0000, Perp-weight  1.056\n",
            "TRAIN Batch 1600/2104, Loss   41.0539, NLL-Loss   38.4342, KL-Loss    2.7486, KL-Weight  0.953, Perp-loss    0.0000, Perp-weight  1.049\n",
            "TRAIN Batch 1650/2104, Loss   53.2799, NLL-Loss   50.3500, KL-Loss    3.0571, KL-Weight  0.958, Perp-loss    0.0000, Perp-weight  1.043\n",
            "TRAIN Batch 1700/2104, Loss   47.1389, NLL-Loss   44.4041, KL-Loss    2.8395, KL-Weight  0.963, Perp-loss    0.0000, Perp-weight  1.038\n",
            "TRAIN Batch 1750/2104, Loss   47.2344, NLL-Loss   44.5766, KL-Loss    2.7477, KL-Weight  0.967, Perp-loss    0.0000, Perp-weight  1.034\n",
            "TRAIN Batch 1800/2104, Loss   47.7791, NLL-Loss   44.8492, KL-Loss    3.0173, KL-Weight  0.971, Perp-loss    0.0000, Perp-weight  1.030\n",
            "TRAIN Batch 1850/2104, Loss   57.9097, NLL-Loss   55.2653, KL-Loss    2.7140, KL-Weight  0.974, Perp-loss    0.0000, Perp-weight  1.026\n",
            "TRAIN Batch 1900/2104, Loss   46.9599, NLL-Loss   44.5419, KL-Loss    2.4742, KL-Weight  0.977, Perp-loss    0.0000, Perp-weight  1.023\n",
            "TRAIN Batch 1950/2104, Loss   37.8476, NLL-Loss   35.4237, KL-Loss    2.4736, KL-Weight  0.980, Perp-loss    0.0000, Perp-weight  1.020\n",
            "TRAIN Batch 2000/2104, Loss   43.6712, NLL-Loss   40.9532, KL-Loss    2.7672, KL-Weight  0.982, Perp-loss    0.0000, Perp-weight  1.018\n",
            "TRAIN Batch 2050/2104, Loss   49.6279, NLL-Loss   46.8060, KL-Loss    2.8669, KL-Weight  0.984, Perp-loss    0.0000, Perp-weight  1.016\n",
            "TRAIN Batch 2100/2104, Loss   48.1203, NLL-Loss   45.2890, KL-Loss    2.8712, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "TRAIN Batch 2104/2104, Loss   50.8856, NLL-Loss   48.2484, KL-Loss    2.6740, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "TRAIN Epoch 01/10, Mean ELBO   49.7286\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E1.pytorch\n",
            "VALID Batch 0000/27, Loss  113.5661, NLL-Loss  109.8393, KL-Loss    3.7787, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "VALID Batch 0027/27, Loss   92.4172, NLL-Loss   88.5644, KL-Loss    3.9064, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "VALID Epoch 01/10, Mean ELBO  117.5791\n",
            "TRAIN Batch 0000/2104, Loss   47.3852, NLL-Loss   44.8140, KL-Loss    2.6070, KL-Weight  0.986, Perp-loss    0.0000, Perp-weight  1.014\n",
            "TRAIN Batch 0050/2104, Loss   47.2862, NLL-Loss   44.6180, KL-Loss    2.7009, KL-Weight  0.988, Perp-loss    0.0000, Perp-weight  1.012\n",
            "TRAIN Batch 0100/2104, Loss   41.4383, NLL-Loss   38.7221, KL-Loss    2.7456, KL-Weight  0.989, Perp-loss    0.0000, Perp-weight  1.011\n",
            "TRAIN Batch 0150/2104, Loss   51.7569, NLL-Loss   48.9425, KL-Loss    2.8414, KL-Weight  0.991, Perp-loss    0.0000, Perp-weight  1.010\n",
            "TRAIN Batch 0200/2104, Loss   38.5209, NLL-Loss   36.1854, KL-Loss    2.3553, KL-Weight  0.992, Perp-loss    0.0000, Perp-weight  1.008\n",
            "TRAIN Batch 0250/2104, Loss   41.5036, NLL-Loss   38.9920, KL-Loss    2.5303, KL-Weight  0.993, Perp-loss    0.0000, Perp-weight  1.007\n",
            "TRAIN Batch 0300/2104, Loss   49.5783, NLL-Loss   47.1397, KL-Loss    2.4546, KL-Weight  0.993, Perp-loss    0.0000, Perp-weight  1.007\n",
            "TRAIN Batch 0350/2104, Loss   53.0875, NLL-Loss   50.1574, KL-Loss    2.9471, KL-Weight  0.994, Perp-loss    0.0000, Perp-weight  1.006\n",
            "TRAIN Batch 0400/2104, Loss   44.1786, NLL-Loss   41.4835, KL-Loss    2.7089, KL-Weight  0.995, Perp-loss    0.0000, Perp-weight  1.005\n",
            "TRAIN Batch 0450/2104, Loss   56.9108, NLL-Loss   54.0537, KL-Loss    2.8700, KL-Weight  0.996, Perp-loss    0.0000, Perp-weight  1.005\n",
            "TRAIN Batch 0500/2104, Loss   42.8371, NLL-Loss   40.3394, KL-Loss    2.5076, KL-Weight  0.996, Perp-loss    0.0000, Perp-weight  1.004\n",
            "TRAIN Batch 0550/2104, Loss   38.9629, NLL-Loss   36.5576, KL-Loss    2.4137, KL-Weight  0.996, Perp-loss    0.0000, Perp-weight  1.004\n",
            "TRAIN Batch 0600/2104, Loss   37.5405, NLL-Loss   35.1747, KL-Loss    2.3731, KL-Weight  0.997, Perp-loss    0.0000, Perp-weight  1.003\n",
            "TRAIN Batch 0650/2104, Loss   36.6040, NLL-Loss   34.1064, KL-Loss    2.5044, KL-Weight  0.997, Perp-loss    0.0000, Perp-weight  1.003\n",
            "TRAIN Batch 0700/2104, Loss   46.2805, NLL-Loss   43.8550, KL-Loss    2.4314, KL-Weight  0.998, Perp-loss    0.0000, Perp-weight  1.002\n",
            "TRAIN Batch 0750/2104, Loss   40.8366, NLL-Loss   38.4818, KL-Loss    2.3598, KL-Weight  0.998, Perp-loss    0.0000, Perp-weight  1.002\n",
            "TRAIN Batch 0800/2104, Loss   65.3586, NLL-Loss   62.4652, KL-Loss    2.8989, KL-Weight  0.998, Perp-loss    0.0000, Perp-weight  1.002\n",
            "TRAIN Batch 0850/2104, Loss   48.2397, NLL-Loss   45.5075, KL-Loss    2.7367, KL-Weight  0.998, Perp-loss    0.0000, Perp-weight  1.002\n",
            "TRAIN Batch 0900/2104, Loss   35.6489, NLL-Loss   33.2446, KL-Loss    2.4078, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 0950/2104, Loss   50.0604, NLL-Loss   47.4460, KL-Loss    2.6177, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1000/2104, Loss   54.5166, NLL-Loss   51.9976, KL-Loss    2.5219, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1050/2104, Loss   49.3507, NLL-Loss   46.8099, KL-Loss    2.5433, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1100/2104, Loss   33.1886, NLL-Loss   30.9006, KL-Loss    2.2901, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1150/2104, Loss   57.7022, NLL-Loss   54.8227, KL-Loss    2.8818, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1200/2104, Loss   40.3548, NLL-Loss   38.1725, KL-Loss    2.1838, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1250/2104, Loss   48.6568, NLL-Loss   46.1442, KL-Loss    2.5141, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1300/2104, Loss   36.1341, NLL-Loss   34.0147, KL-Loss    2.1205, KL-Weight  0.999, Perp-loss    0.0000, Perp-weight  1.001\n",
            "TRAIN Batch 1350/2104, Loss   40.9719, NLL-Loss   38.6532, KL-Loss    2.3198, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   41.2027, NLL-Loss   38.9467, KL-Loss    2.2570, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   41.7704, NLL-Loss   39.4065, KL-Loss    2.3647, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   41.8768, NLL-Loss   39.8079, KL-Loss    2.0696, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   52.3698, NLL-Loss   49.7449, KL-Loss    2.6257, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   46.7968, NLL-Loss   44.2846, KL-Loss    2.5128, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   55.1291, NLL-Loss   52.5202, KL-Loss    2.6094, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   46.2249, NLL-Loss   43.9599, KL-Loss    2.2654, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   44.7273, NLL-Loss   42.3816, KL-Loss    2.3461, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   42.0196, NLL-Loss   39.6728, KL-Loss    2.3471, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   44.1236, NLL-Loss   41.6369, KL-Loss    2.4871, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   40.8746, NLL-Loss   38.6870, KL-Loss    2.1879, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   45.8372, NLL-Loss   43.4631, KL-Loss    2.3743, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   39.6130, NLL-Loss   37.4430, KL-Loss    2.1702, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   45.1557, NLL-Loss   42.6710, KL-Loss    2.4848, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   38.4473, NLL-Loss   36.3303, KL-Loss    2.1171, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   36.3322, NLL-Loss   34.1882, KL-Loss    2.1442, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 02/10, Mean ELBO   44.3241\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E2.pytorch\n",
            "VALID Batch 0000/27, Loss  115.3552, NLL-Loss  112.0046, KL-Loss    3.3508, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss   94.4314, NLL-Loss   90.8373, KL-Loss    3.5944, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 02/10, Mean ELBO  120.0328\n",
            "TRAIN Batch 0000/2104, Loss   42.9144, NLL-Loss   40.5588, KL-Loss    2.3558, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   44.2999, NLL-Loss   42.0061, KL-Loss    2.2939, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   33.8153, NLL-Loss   31.7758, KL-Loss    2.0396, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   39.8725, NLL-Loss   37.4116, KL-Loss    2.4610, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   50.7141, NLL-Loss   48.0834, KL-Loss    2.6308, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   42.9680, NLL-Loss   40.6749, KL-Loss    2.2932, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   49.7747, NLL-Loss   47.2601, KL-Loss    2.5147, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   39.8069, NLL-Loss   37.3319, KL-Loss    2.4751, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   45.4181, NLL-Loss   42.9702, KL-Loss    2.4480, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   43.8030, NLL-Loss   41.4889, KL-Loss    2.3141, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   37.8704, NLL-Loss   35.6508, KL-Loss    2.2196, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   29.4507, NLL-Loss   27.4881, KL-Loss    1.9626, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   37.8084, NLL-Loss   35.5301, KL-Loss    2.2783, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   43.2505, NLL-Loss   40.8853, KL-Loss    2.3652, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   47.6883, NLL-Loss   45.3827, KL-Loss    2.3056, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   48.0498, NLL-Loss   45.5941, KL-Loss    2.4557, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   38.3695, NLL-Loss   36.0716, KL-Loss    2.2979, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   43.9107, NLL-Loss   41.3771, KL-Loss    2.5337, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   39.8662, NLL-Loss   37.6610, KL-Loss    2.2052, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   36.0663, NLL-Loss   33.9896, KL-Loss    2.0767, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   36.3484, NLL-Loss   34.1217, KL-Loss    2.2267, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   48.1757, NLL-Loss   45.7613, KL-Loss    2.4144, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   46.5788, NLL-Loss   44.2896, KL-Loss    2.2892, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   34.0640, NLL-Loss   32.0959, KL-Loss    1.9681, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   51.2736, NLL-Loss   49.0044, KL-Loss    2.2692, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   41.6746, NLL-Loss   39.4344, KL-Loss    2.2402, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   36.4789, NLL-Loss   34.3287, KL-Loss    2.1503, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   47.3405, NLL-Loss   44.8699, KL-Loss    2.4706, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   47.6173, NLL-Loss   45.2901, KL-Loss    2.3272, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   42.2273, NLL-Loss   40.1229, KL-Loss    2.1044, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   33.8527, NLL-Loss   31.8435, KL-Loss    2.0092, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   44.4807, NLL-Loss   42.2179, KL-Loss    2.2628, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   39.9568, NLL-Loss   37.7812, KL-Loss    2.1755, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   36.2480, NLL-Loss   34.0289, KL-Loss    2.2190, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   46.6308, NLL-Loss   44.4311, KL-Loss    2.1997, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   42.3325, NLL-Loss   40.1797, KL-Loss    2.1527, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   39.2896, NLL-Loss   37.2478, KL-Loss    2.0418, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   40.9083, NLL-Loss   38.7634, KL-Loss    2.1449, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   56.1933, NLL-Loss   53.6328, KL-Loss    2.5605, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   38.0522, NLL-Loss   35.9900, KL-Loss    2.0622, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   44.0012, NLL-Loss   41.7682, KL-Loss    2.2330, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   35.5861, NLL-Loss   33.6962, KL-Loss    1.8899, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   44.5289, NLL-Loss   42.2140, KL-Loss    2.3149, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   35.7567, NLL-Loss   33.7330, KL-Loss    2.0236, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 03/10, Mean ELBO   40.3278\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E3.pytorch\n",
            "VALID Batch 0000/27, Loss  117.1638, NLL-Loss  114.0176, KL-Loss    3.1462, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss   96.2346, NLL-Loss   93.1124, KL-Loss    3.1221, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 03/10, Mean ELBO  122.3865\n",
            "TRAIN Batch 0000/2104, Loss   35.0990, NLL-Loss   33.0724, KL-Loss    2.0265, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   38.3624, NLL-Loss   36.0915, KL-Loss    2.2709, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   38.6111, NLL-Loss   36.3680, KL-Loss    2.2432, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   35.4444, NLL-Loss   33.4765, KL-Loss    1.9679, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   42.6558, NLL-Loss   40.2407, KL-Loss    2.4151, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   35.7156, NLL-Loss   33.4677, KL-Loss    2.2478, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   37.9362, NLL-Loss   35.9257, KL-Loss    2.0105, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   40.3715, NLL-Loss   38.1899, KL-Loss    2.1816, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   37.7924, NLL-Loss   35.5493, KL-Loss    2.2431, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   35.9923, NLL-Loss   33.8285, KL-Loss    2.1638, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   36.2015, NLL-Loss   34.2852, KL-Loss    1.9163, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   35.7808, NLL-Loss   33.9505, KL-Loss    1.8303, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   34.8735, NLL-Loss   32.7530, KL-Loss    2.1205, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   47.1162, NLL-Loss   44.7706, KL-Loss    2.3456, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   44.7341, NLL-Loss   42.4521, KL-Loss    2.2820, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   41.9931, NLL-Loss   39.9659, KL-Loss    2.0272, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   28.2320, NLL-Loss   26.4205, KL-Loss    1.8116, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   34.7582, NLL-Loss   32.7031, KL-Loss    2.0551, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   35.2619, NLL-Loss   33.2567, KL-Loss    2.0051, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   35.6294, NLL-Loss   33.7475, KL-Loss    1.8819, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   37.4009, NLL-Loss   35.1171, KL-Loss    2.2838, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   31.7893, NLL-Loss   30.0619, KL-Loss    1.7274, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   38.1265, NLL-Loss   36.0479, KL-Loss    2.0786, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   35.5323, NLL-Loss   33.2033, KL-Loss    2.3291, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   43.4291, NLL-Loss   41.1447, KL-Loss    2.2845, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   48.4634, NLL-Loss   46.3093, KL-Loss    2.1541, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   43.0969, NLL-Loss   41.0695, KL-Loss    2.0275, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   36.3409, NLL-Loss   34.3946, KL-Loss    1.9463, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   44.2101, NLL-Loss   42.0446, KL-Loss    2.1655, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   36.0151, NLL-Loss   33.9835, KL-Loss    2.0316, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   37.4782, NLL-Loss   35.4987, KL-Loss    1.9795, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   29.2739, NLL-Loss   27.2998, KL-Loss    1.9741, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   40.7396, NLL-Loss   38.5136, KL-Loss    2.2260, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   29.7917, NLL-Loss   28.0291, KL-Loss    1.7626, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   42.9260, NLL-Loss   40.6799, KL-Loss    2.2461, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   40.5063, NLL-Loss   38.4288, KL-Loss    2.0775, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   36.8986, NLL-Loss   34.8649, KL-Loss    2.0336, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   29.7940, NLL-Loss   27.9527, KL-Loss    1.8412, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   37.6912, NLL-Loss   35.5707, KL-Loss    2.1205, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   36.7267, NLL-Loss   34.6123, KL-Loss    2.1144, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   32.4412, NLL-Loss   30.3385, KL-Loss    2.1027, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   46.2484, NLL-Loss   44.1078, KL-Loss    2.1407, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   40.4141, NLL-Loss   38.4082, KL-Loss    2.0059, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   38.3721, NLL-Loss   36.4547, KL-Loss    1.9173, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 04/10, Mean ELBO   37.4956\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E4.pytorch\n",
            "VALID Batch 0000/27, Loss  120.3867, NLL-Loss  117.1811, KL-Loss    3.2056, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss   96.7780, NLL-Loss   93.5223, KL-Loss    3.2557, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 04/10, Mean ELBO  125.2215\n",
            "TRAIN Batch 0000/2104, Loss   27.6024, NLL-Loss   25.6647, KL-Loss    1.9377, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   41.5527, NLL-Loss   39.4646, KL-Loss    2.0881, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   32.3552, NLL-Loss   30.3463, KL-Loss    2.0088, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   32.0606, NLL-Loss   29.9977, KL-Loss    2.0629, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   40.8640, NLL-Loss   38.7387, KL-Loss    2.1253, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   42.3034, NLL-Loss   40.0425, KL-Loss    2.2608, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   28.5861, NLL-Loss   26.5346, KL-Loss    2.0514, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   30.0074, NLL-Loss   27.9793, KL-Loss    2.0281, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   32.7693, NLL-Loss   30.8856, KL-Loss    1.8837, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   31.0578, NLL-Loss   29.0458, KL-Loss    2.0120, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   34.2008, NLL-Loss   32.0527, KL-Loss    2.1482, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   33.2505, NLL-Loss   31.2464, KL-Loss    2.0041, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   33.8120, NLL-Loss   31.6265, KL-Loss    2.1855, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   36.1012, NLL-Loss   34.1248, KL-Loss    1.9764, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   45.2019, NLL-Loss   43.1600, KL-Loss    2.0419, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   44.4718, NLL-Loss   42.3152, KL-Loss    2.1566, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   37.9773, NLL-Loss   35.7311, KL-Loss    2.2463, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   38.5137, NLL-Loss   36.5596, KL-Loss    1.9541, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   38.6436, NLL-Loss   36.5304, KL-Loss    2.1131, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   31.2379, NLL-Loss   29.4616, KL-Loss    1.7762, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   39.6388, NLL-Loss   37.5956, KL-Loss    2.0432, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   36.6567, NLL-Loss   34.6473, KL-Loss    2.0094, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   32.4641, NLL-Loss   30.4378, KL-Loss    2.0263, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   34.0388, NLL-Loss   32.1499, KL-Loss    1.8889, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   34.5573, NLL-Loss   32.7232, KL-Loss    1.8340, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   32.1222, NLL-Loss   30.2756, KL-Loss    1.8466, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   35.4206, NLL-Loss   33.4796, KL-Loss    1.9410, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   37.7100, NLL-Loss   35.7492, KL-Loss    1.9608, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   33.4095, NLL-Loss   31.4725, KL-Loss    1.9369, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   33.2875, NLL-Loss   31.3844, KL-Loss    1.9031, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   37.1196, NLL-Loss   35.0142, KL-Loss    2.1054, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   43.7295, NLL-Loss   41.5777, KL-Loss    2.1519, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   35.4499, NLL-Loss   33.5105, KL-Loss    1.9394, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   26.7341, NLL-Loss   24.9555, KL-Loss    1.7786, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   36.7392, NLL-Loss   34.7141, KL-Loss    2.0251, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   45.7356, NLL-Loss   43.5244, KL-Loss    2.2112, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   30.1561, NLL-Loss   28.3354, KL-Loss    1.8207, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   40.4856, NLL-Loss   38.2924, KL-Loss    2.1933, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   37.6259, NLL-Loss   35.6084, KL-Loss    2.0175, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   30.7960, NLL-Loss   29.0701, KL-Loss    1.7259, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   39.0943, NLL-Loss   37.0775, KL-Loss    2.0167, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   29.4197, NLL-Loss   27.5176, KL-Loss    1.9021, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   35.4151, NLL-Loss   33.2529, KL-Loss    2.1622, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   36.0474, NLL-Loss   33.9495, KL-Loss    2.0979, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 05/10, Mean ELBO   35.4085\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E5.pytorch\n",
            "VALID Batch 0000/27, Loss  121.9796, NLL-Loss  118.9925, KL-Loss    2.9871, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss   99.4635, NLL-Loss   96.4647, KL-Loss    2.9987, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 05/10, Mean ELBO  127.9411\n",
            "TRAIN Batch 0000/2104, Loss   30.4266, NLL-Loss   28.5230, KL-Loss    1.9036, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   30.5279, NLL-Loss   28.7698, KL-Loss    1.7580, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   28.6353, NLL-Loss   26.7288, KL-Loss    1.9065, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   35.0522, NLL-Loss   32.9854, KL-Loss    2.0667, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   31.4473, NLL-Loss   29.4295, KL-Loss    2.0179, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   34.8185, NLL-Loss   32.7553, KL-Loss    2.0632, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   33.6234, NLL-Loss   31.6968, KL-Loss    1.9265, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   35.2111, NLL-Loss   33.2626, KL-Loss    1.9486, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   38.0694, NLL-Loss   36.1478, KL-Loss    1.9216, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   31.4384, NLL-Loss   29.3841, KL-Loss    2.0542, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   31.0341, NLL-Loss   28.9612, KL-Loss    2.0729, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   39.7832, NLL-Loss   37.2709, KL-Loss    2.5124, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   36.3448, NLL-Loss   34.3154, KL-Loss    2.0294, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   27.6647, NLL-Loss   25.8216, KL-Loss    1.8431, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   32.2566, NLL-Loss   30.3691, KL-Loss    1.8874, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   26.9272, NLL-Loss   24.9775, KL-Loss    1.9497, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   28.6779, NLL-Loss   26.8872, KL-Loss    1.7907, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   37.8652, NLL-Loss   35.6920, KL-Loss    2.1732, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   28.4303, NLL-Loss   26.5730, KL-Loss    1.8573, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   31.4212, NLL-Loss   29.4051, KL-Loss    2.0160, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   40.6693, NLL-Loss   38.6726, KL-Loss    1.9967, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   32.2335, NLL-Loss   30.1551, KL-Loss    2.0784, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   33.5284, NLL-Loss   31.7135, KL-Loss    1.8149, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   42.7830, NLL-Loss   40.6736, KL-Loss    2.1093, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   24.8109, NLL-Loss   23.2718, KL-Loss    1.5392, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   30.8531, NLL-Loss   28.7715, KL-Loss    2.0815, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   36.9039, NLL-Loss   34.9241, KL-Loss    1.9798, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   37.6854, NLL-Loss   35.5170, KL-Loss    2.1685, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   29.3571, NLL-Loss   27.4976, KL-Loss    1.8595, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   34.2067, NLL-Loss   31.8876, KL-Loss    2.3191, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   31.1695, NLL-Loss   29.3481, KL-Loss    1.8215, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   31.5730, NLL-Loss   29.8823, KL-Loss    1.6907, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   33.8649, NLL-Loss   32.1244, KL-Loss    1.7405, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   36.8867, NLL-Loss   34.8012, KL-Loss    2.0855, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   32.1312, NLL-Loss   30.3271, KL-Loss    1.8041, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   41.4264, NLL-Loss   39.1228, KL-Loss    2.3036, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   33.6440, NLL-Loss   31.7459, KL-Loss    1.8982, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   33.6785, NLL-Loss   31.7729, KL-Loss    1.9056, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   36.5814, NLL-Loss   34.7404, KL-Loss    1.8409, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   34.7238, NLL-Loss   32.8801, KL-Loss    1.8437, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   36.4283, NLL-Loss   34.3149, KL-Loss    2.1134, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   31.2530, NLL-Loss   29.4463, KL-Loss    1.8067, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   33.2073, NLL-Loss   31.1506, KL-Loss    2.0567, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   29.6274, NLL-Loss   27.9402, KL-Loss    1.6872, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 06/10, Mean ELBO   33.7560\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E6.pytorch\n",
            "VALID Batch 0000/27, Loss  125.5246, NLL-Loss  122.5825, KL-Loss    2.9421, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss  101.2004, NLL-Loss   98.1121, KL-Loss    3.0883, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 06/10, Mean ELBO  130.5326\n",
            "TRAIN Batch 0000/2104, Loss   30.9520, NLL-Loss   28.7962, KL-Loss    2.1558, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   32.5132, NLL-Loss   30.8385, KL-Loss    1.6746, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   34.4086, NLL-Loss   32.5635, KL-Loss    1.8451, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   27.1595, NLL-Loss   25.3501, KL-Loss    1.8094, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   34.2038, NLL-Loss   32.2241, KL-Loss    1.9797, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   29.1444, NLL-Loss   27.3819, KL-Loss    1.7625, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   29.4484, NLL-Loss   27.6710, KL-Loss    1.7774, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   29.7914, NLL-Loss   27.8192, KL-Loss    1.9722, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   29.7099, NLL-Loss   27.7086, KL-Loss    2.0014, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   41.4717, NLL-Loss   39.3464, KL-Loss    2.1254, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   32.2322, NLL-Loss   30.2845, KL-Loss    1.9477, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   33.4223, NLL-Loss   31.4296, KL-Loss    1.9926, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   35.8813, NLL-Loss   33.9391, KL-Loss    1.9423, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   31.4518, NLL-Loss   29.4899, KL-Loss    1.9619, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   28.9020, NLL-Loss   27.1964, KL-Loss    1.7057, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   32.7607, NLL-Loss   30.7927, KL-Loss    1.9680, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   31.6972, NLL-Loss   29.7586, KL-Loss    1.9386, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   29.5469, NLL-Loss   27.5819, KL-Loss    1.9650, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   28.9825, NLL-Loss   27.1412, KL-Loss    1.8413, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   34.7351, NLL-Loss   32.9556, KL-Loss    1.7796, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   33.4447, NLL-Loss   31.5542, KL-Loss    1.8905, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   38.0798, NLL-Loss   36.1131, KL-Loss    1.9667, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   39.0204, NLL-Loss   37.0030, KL-Loss    2.0174, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   37.1214, NLL-Loss   35.2601, KL-Loss    1.8613, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   31.3516, NLL-Loss   29.3251, KL-Loss    2.0266, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   30.0639, NLL-Loss   28.1131, KL-Loss    1.9508, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   32.8028, NLL-Loss   30.9627, KL-Loss    1.8401, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   30.2796, NLL-Loss   28.5229, KL-Loss    1.7568, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   30.4658, NLL-Loss   28.5781, KL-Loss    1.8877, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   30.3944, NLL-Loss   28.4018, KL-Loss    1.9927, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   36.1942, NLL-Loss   34.2282, KL-Loss    1.9659, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   28.4633, NLL-Loss   26.7228, KL-Loss    1.7405, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   33.2044, NLL-Loss   31.2288, KL-Loss    1.9756, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   34.2041, NLL-Loss   32.2588, KL-Loss    1.9453, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   26.6229, NLL-Loss   24.9700, KL-Loss    1.6529, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   37.9695, NLL-Loss   35.7861, KL-Loss    2.1834, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   30.3645, NLL-Loss   28.5645, KL-Loss    1.8000, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   43.1808, NLL-Loss   41.0102, KL-Loss    2.1706, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   26.3343, NLL-Loss   24.7507, KL-Loss    1.5836, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   36.6599, NLL-Loss   34.7418, KL-Loss    1.9181, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   26.2888, NLL-Loss   24.4639, KL-Loss    1.8250, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   32.9109, NLL-Loss   30.9317, KL-Loss    1.9792, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   23.8445, NLL-Loss   22.0888, KL-Loss    1.7556, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   37.2382, NLL-Loss   35.4196, KL-Loss    1.8186, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 07/10, Mean ELBO   32.4204\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E7.pytorch\n",
            "VALID Batch 0000/27, Loss  127.8126, NLL-Loss  125.2820, KL-Loss    2.5306, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss  105.4563, NLL-Loss  102.8436, KL-Loss    2.6126, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 07/10, Mean ELBO  132.3727\n",
            "TRAIN Batch 0000/2104, Loss   28.1715, NLL-Loss   26.3892, KL-Loss    1.7823, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   32.3941, NLL-Loss   30.7445, KL-Loss    1.6496, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   30.7397, NLL-Loss   28.8031, KL-Loss    1.9365, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   31.1313, NLL-Loss   29.5003, KL-Loss    1.6310, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   28.1173, NLL-Loss   26.3315, KL-Loss    1.7857, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   36.0265, NLL-Loss   33.8720, KL-Loss    2.1545, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   32.2302, NLL-Loss   30.0065, KL-Loss    2.2237, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   34.2189, NLL-Loss   32.2506, KL-Loss    1.9683, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   30.3137, NLL-Loss   28.4844, KL-Loss    1.8294, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   25.1025, NLL-Loss   23.3792, KL-Loss    1.7233, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   27.5022, NLL-Loss   25.9204, KL-Loss    1.5818, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   33.5060, NLL-Loss   31.3992, KL-Loss    2.1068, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   33.0060, NLL-Loss   31.0402, KL-Loss    1.9658, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   28.7679, NLL-Loss   26.9812, KL-Loss    1.7867, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   28.5647, NLL-Loss   26.8121, KL-Loss    1.7526, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   36.1328, NLL-Loss   34.0261, KL-Loss    2.1067, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   34.3173, NLL-Loss   32.2509, KL-Loss    2.0664, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   30.5803, NLL-Loss   28.7855, KL-Loss    1.7948, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   33.9520, NLL-Loss   31.9828, KL-Loss    1.9692, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   34.1594, NLL-Loss   32.3457, KL-Loss    1.8138, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   34.1321, NLL-Loss   32.1858, KL-Loss    1.9463, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   26.5671, NLL-Loss   24.9805, KL-Loss    1.5866, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   27.0950, NLL-Loss   25.3028, KL-Loss    1.7921, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   33.4423, NLL-Loss   31.6369, KL-Loss    1.8055, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   26.1965, NLL-Loss   24.5007, KL-Loss    1.6958, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   30.2961, NLL-Loss   28.7261, KL-Loss    1.5700, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   28.0216, NLL-Loss   26.2364, KL-Loss    1.7852, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   33.6275, NLL-Loss   31.9680, KL-Loss    1.6595, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   28.5622, NLL-Loss   26.8090, KL-Loss    1.7532, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   26.6528, NLL-Loss   24.9623, KL-Loss    1.6906, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   32.0439, NLL-Loss   30.2292, KL-Loss    1.8147, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   31.4898, NLL-Loss   29.6376, KL-Loss    1.8523, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   34.1261, NLL-Loss   32.2739, KL-Loss    1.8522, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   30.1677, NLL-Loss   28.4759, KL-Loss    1.6917, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   27.4255, NLL-Loss   25.6519, KL-Loss    1.7735, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   31.5771, NLL-Loss   29.8196, KL-Loss    1.7575, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   27.8164, NLL-Loss   26.0991, KL-Loss    1.7172, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   35.3272, NLL-Loss   33.5289, KL-Loss    1.7983, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   29.1402, NLL-Loss   27.3152, KL-Loss    1.8250, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   26.4558, NLL-Loss   24.7961, KL-Loss    1.6597, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   27.4462, NLL-Loss   25.6177, KL-Loss    1.8285, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   30.3211, NLL-Loss   28.7336, KL-Loss    1.5875, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   29.6585, NLL-Loss   27.9846, KL-Loss    1.6739, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   35.2432, NLL-Loss   33.3859, KL-Loss    1.8573, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 08/10, Mean ELBO   31.2981\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E8.pytorch\n",
            "VALID Batch 0000/27, Loss  128.8703, NLL-Loss  126.1477, KL-Loss    2.7226, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss  103.3714, NLL-Loss  100.6485, KL-Loss    2.7230, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 08/10, Mean ELBO  134.7749\n",
            "TRAIN Batch 0000/2104, Loss   29.5090, NLL-Loss   27.6134, KL-Loss    1.8955, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0050/2104, Loss   31.0194, NLL-Loss   29.1419, KL-Loss    1.8775, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0100/2104, Loss   29.6847, NLL-Loss   27.9086, KL-Loss    1.7762, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0150/2104, Loss   34.8969, NLL-Loss   33.0652, KL-Loss    1.8317, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0200/2104, Loss   36.6363, NLL-Loss   34.3956, KL-Loss    2.2407, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0250/2104, Loss   26.1739, NLL-Loss   24.4681, KL-Loss    1.7058, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0300/2104, Loss   28.6240, NLL-Loss   26.8338, KL-Loss    1.7902, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0350/2104, Loss   26.7790, NLL-Loss   25.1378, KL-Loss    1.6412, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0400/2104, Loss   31.8706, NLL-Loss   30.1587, KL-Loss    1.7119, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0450/2104, Loss   35.0529, NLL-Loss   32.9831, KL-Loss    2.0698, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0500/2104, Loss   28.9356, NLL-Loss   27.0787, KL-Loss    1.8569, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0550/2104, Loss   30.8643, NLL-Loss   29.0670, KL-Loss    1.7973, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0600/2104, Loss   36.3041, NLL-Loss   34.4324, KL-Loss    1.8717, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0650/2104, Loss   29.6860, NLL-Loss   27.7190, KL-Loss    1.9671, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0700/2104, Loss   23.7429, NLL-Loss   22.1860, KL-Loss    1.5570, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0750/2104, Loss   33.4390, NLL-Loss   31.4872, KL-Loss    1.9517, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0800/2104, Loss   30.4824, NLL-Loss   28.7997, KL-Loss    1.6828, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0850/2104, Loss   38.3541, NLL-Loss   36.2034, KL-Loss    2.1507, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0900/2104, Loss   24.3906, NLL-Loss   22.6730, KL-Loss    1.7176, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 0950/2104, Loss   26.4070, NLL-Loss   24.6476, KL-Loss    1.7594, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1000/2104, Loss   31.0506, NLL-Loss   29.2839, KL-Loss    1.7667, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1050/2104, Loss   28.0263, NLL-Loss   26.2716, KL-Loss    1.7547, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1100/2104, Loss   28.4364, NLL-Loss   26.7179, KL-Loss    1.7185, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1150/2104, Loss   29.5249, NLL-Loss   27.9027, KL-Loss    1.6222, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1200/2104, Loss   29.4543, NLL-Loss   27.5910, KL-Loss    1.8633, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1250/2104, Loss   30.3257, NLL-Loss   28.5928, KL-Loss    1.7329, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1300/2104, Loss   30.8751, NLL-Loss   29.1021, KL-Loss    1.7729, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1350/2104, Loss   27.4032, NLL-Loss   25.6618, KL-Loss    1.7413, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1400/2104, Loss   27.3406, NLL-Loss   25.6862, KL-Loss    1.6545, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1450/2104, Loss   33.0086, NLL-Loss   31.2404, KL-Loss    1.7682, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1500/2104, Loss   28.0013, NLL-Loss   26.0705, KL-Loss    1.9308, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1550/2104, Loss   29.2229, NLL-Loss   27.3632, KL-Loss    1.8597, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1600/2104, Loss   35.2966, NLL-Loss   33.4486, KL-Loss    1.8481, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1650/2104, Loss   32.6661, NLL-Loss   30.8902, KL-Loss    1.7759, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1700/2104, Loss   29.5235, NLL-Loss   27.5625, KL-Loss    1.9610, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1750/2104, Loss   34.9164, NLL-Loss   33.0539, KL-Loss    1.8625, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1800/2104, Loss   30.5429, NLL-Loss   28.6226, KL-Loss    1.9204, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1850/2104, Loss   27.7888, NLL-Loss   26.1260, KL-Loss    1.6628, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1900/2104, Loss   28.2220, NLL-Loss   26.4849, KL-Loss    1.7371, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 1950/2104, Loss   34.9342, NLL-Loss   33.0744, KL-Loss    1.8598, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2000/2104, Loss   31.3062, NLL-Loss   29.4197, KL-Loss    1.8865, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2050/2104, Loss   29.5456, NLL-Loss   27.6866, KL-Loss    1.8590, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2100/2104, Loss   33.0266, NLL-Loss   31.3279, KL-Loss    1.6988, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Batch 2104/2104, Loss   27.8360, NLL-Loss   26.0204, KL-Loss    1.8156, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "TRAIN Epoch 09/10, Mean ELBO   30.3749\n",
            "Model saved at bin/2020-Dec-07-21:43:33/E9.pytorch\n",
            "VALID Batch 0000/27, Loss  131.4679, NLL-Loss  128.7295, KL-Loss    2.7384, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Batch 0027/27, Loss  103.6126, NLL-Loss  100.8314, KL-Loss    2.7812, KL-Weight  1.000, Perp-loss    0.0000, Perp-weight  1.000\n",
            "VALID Epoch 09/10, Mean ELBO  136.7958\n",
            "Total train time: 1151.4904823303223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-T51bhAMdSt"
      },
      "source": [
        "Test your model with an adversarial attack, please replace `bin/2020-Dec-07-21:43:33/E9.pytorch` with the path to your checkpoint as shown in the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhYAOMwmKNfI",
        "outputId": "9016b8da-22f8-40d3-dff0-62a37175091e"
      },
      "source": [
        "!python semi_attack.py -c bin/2020-Dec-07-21:43:33/E9.pytorch --rnn_type lstm --iter 2 --steps 10 --rseed 7 --most_similar -v \\\n",
        "  --victim_model \"distilbert-base-uncased-finetuned-sst-2-english\" \\\n",
        "  --victim_sentence \"a strangely compelling and brilliantly acted psychological drama .\" \\\n",
        "  --reference_sentence \"an absurdist sitcom about alienation , separation and loss .\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-07 22:35:09.916066: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Model loaded from bin/2020-Dec-07-21:43:33/E9.pytorch\n",
            "\n",
            "-------Initial Inputs-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688\n",
            "Reference Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "\n",
            "-------ITERATION 0-------\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . \\\\\n",
            "1.000 & , it ' s a very good yarn .  \\\\\n",
            "1.000 & , it ' s a very good viewing alternative .  \\\\\n",
            "0.998 & a fascinating curiosity piece of filmmaking  \\\\\n",
            "0.003 & an absurdist sitcom about alienation , separation and loss . \\\\\n",
            "\n",
            "-------ITERATION 1-------\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n",
            "-------PREDICTIONS-------\n",
            "1.000 & a strangely compelling and brilliantly acted psychological drama . \\\\\n",
            "0.999 & a fascinating curiosity piece of filmmaking , and the class warfare that embroils two surefire men  \\\\\n",
            "0.998 & a fascinating curiosity piece of filmmaking , and  \\\\\n",
            "0.998 & a fascinating curiosity piece of filmmaking ,  \\\\\n",
            "0.998 & a fascinating curiosity piece of filmmaking  \\\\\n",
            "0.003 & an absurdist sitcom about alienation , separation and loss . \\\\\n",
            "-------Attack Result-------\n",
            "Victim Sentence: a strangely compelling and brilliantly acted psychological drama . pred:0.999883770942688\n",
            "Best Adv Sentence: an absurdist sitcom about alienation , separation and loss . pred:0.00250418484210968\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}